{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5ltlHrJlmAd"
      },
      "outputs": [],
      "source": [
        "import torch, time, gc\n",
        "\n",
        "# Timing utilities\n",
        "start_time = None\n",
        "\n",
        "def start_timer():\n",
        "    global start_time\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_max_memory_allocated()\n",
        "    torch.cuda.synchronize()\n",
        "    start_time = time.time()\n",
        "\n",
        "def end_timer_and_print(local_msg):\n",
        "    torch.cuda.synchronize()\n",
        "    end_time = time.time()\n",
        "    print(\"\\n\" + local_msg)\n",
        "    print(\"Total execution time = {:.3f} sec\".format(end_time - start_time))\n",
        "    print(\"Max memory used by tensors = {} bytes\".format(torch.cuda.max_memory_allocated()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iLH414Y5gh-",
        "outputId": "a11800c9-c52b-4c70-aa31-da7eeb0c0bb2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:329: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Step [100/500], Loss: 6.9183\n",
            "Epoch [1/5], Step [200/500], Loss: 6.5926\n",
            "Epoch [1/5], Step [300/500], Loss: 6.2373\n",
            "Epoch [1/5], Step [400/500], Loss: 5.9018\n",
            "Epoch [1/5], Step [500/500], Loss: 5.5489\n",
            "Epoch [2/5], Step [100/500], Loss: 5.1552\n",
            "Epoch [2/5], Step [200/500], Loss: 4.6138\n",
            "Epoch [2/5], Step [300/500], Loss: 4.2194\n",
            "Epoch [2/5], Step [400/500], Loss: 4.2819\n",
            "Epoch [2/5], Step [500/500], Loss: 3.8655\n",
            "Epoch [3/5], Step [100/500], Loss: 3.5685\n",
            "Epoch [3/5], Step [200/500], Loss: 3.3137\n",
            "Epoch [3/5], Step [300/500], Loss: 3.1518\n",
            "Epoch [3/5], Step [400/500], Loss: 2.9987\n",
            "Epoch [3/5], Step [500/500], Loss: 2.8093\n",
            "Epoch [4/5], Step [100/500], Loss: 2.8529\n",
            "Epoch [4/5], Step [200/500], Loss: 2.4370\n",
            "Epoch [4/5], Step [300/500], Loss: 2.5947\n",
            "Epoch [4/5], Step [400/500], Loss: 2.4655\n",
            "Epoch [4/5], Step [500/500], Loss: 2.3294\n",
            "Epoch [5/5], Step [100/500], Loss: 2.2491\n",
            "Epoch [5/5], Step [200/500], Loss: 2.3896\n",
            "Epoch [5/5], Step [300/500], Loss: 2.1843\n",
            "Epoch [5/5], Step [400/500], Loss: 2.0888\n",
            "Epoch [5/5], Step [500/500], Loss: 2.2192\n",
            "\n",
            "Standart precision:\n",
            "Total execution time = 63.054 sec\n",
            "Max memory used by tensors = 166830080 bytes\n",
            "Files already downloaded and verified\n",
            "Accuracy of the network on the 10000 test images: 32 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Epoch [1/5], Step [100/500], Loss: 6.9275\n",
            "Epoch [1/5], Step [200/500], Loss: 6.5791\n",
            "Epoch [1/5], Step [300/500], Loss: 6.2466\n",
            "Epoch [1/5], Step [400/500], Loss: 5.8921\n",
            "Epoch [1/5], Step [500/500], Loss: 5.5237\n",
            "Epoch [2/5], Step [100/500], Loss: 5.1916\n",
            "Epoch [2/5], Step [200/500], Loss: 4.6644\n",
            "Epoch [2/5], Step [300/500], Loss: 4.2550\n",
            "Epoch [2/5], Step [400/500], Loss: 4.2432\n",
            "Epoch [2/5], Step [500/500], Loss: 3.9107\n",
            "Epoch [3/5], Step [100/500], Loss: 3.5985\n",
            "Epoch [3/5], Step [200/500], Loss: 3.3166\n",
            "Epoch [3/5], Step [300/500], Loss: 3.1856\n",
            "Epoch [3/5], Step [400/500], Loss: 2.9745\n",
            "Epoch [3/5], Step [500/500], Loss: 2.8188\n",
            "Epoch [4/5], Step [100/500], Loss: 2.8825\n",
            "Epoch [4/5], Step [200/500], Loss: 2.4484\n",
            "Epoch [4/5], Step [300/500], Loss: 2.5785\n",
            "Epoch [4/5], Step [400/500], Loss: 2.5000\n",
            "Epoch [4/5], Step [500/500], Loss: 2.3009\n",
            "Epoch [5/5], Step [100/500], Loss: 2.1745\n",
            "Epoch [5/5], Step [200/500], Loss: 2.3812\n",
            "Epoch [5/5], Step [300/500], Loss: 2.1984\n",
            "Epoch [5/5], Step [400/500], Loss: 2.0865\n",
            "Epoch [5/5], Step [500/500], Loss: 2.2274\n",
            "\n",
            "Mixed precision:\n",
            "Total execution time = 66.776 sec\n",
            "Max memory used by tensors = 139232256 bytes\n",
            "Files already downloaded and verified\n",
            "Accuracy of the network on the 10000 test images: 32 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Epoch [1/5], Step [100/500], Loss: 7.0600\n",
            "Epoch [1/5], Step [200/500], Loss: 6.8269\n",
            "Epoch [1/5], Step [300/500], Loss: 6.5725\n",
            "Epoch [1/5], Step [400/500], Loss: 6.4223\n",
            "Epoch [1/5], Step [500/500], Loss: 6.1407\n",
            "Epoch [2/5], Step [100/500], Loss: 5.9111\n",
            "Epoch [2/5], Step [200/500], Loss: 5.5568\n",
            "Epoch [2/5], Step [300/500], Loss: 5.2705\n",
            "Epoch [2/5], Step [400/500], Loss: 5.1285\n",
            "Epoch [2/5], Step [500/500], Loss: 5.0193\n",
            "Epoch [3/5], Step [100/500], Loss: 4.6782\n",
            "Epoch [3/5], Step [200/500], Loss: 4.4520\n",
            "Epoch [3/5], Step [300/500], Loss: 4.3200\n",
            "Epoch [3/5], Step [400/500], Loss: 4.1896\n",
            "Epoch [3/5], Step [500/500], Loss: 4.0132\n",
            "Epoch [4/5], Step [100/500], Loss: 3.9109\n",
            "Epoch [4/5], Step [200/500], Loss: 3.5569\n",
            "Epoch [4/5], Step [300/500], Loss: 3.5630\n",
            "Epoch [4/5], Step [400/500], Loss: 3.4555\n",
            "Epoch [4/5], Step [500/500], Loss: 3.3631\n",
            "Epoch [5/5], Step [100/500], Loss: 3.3713\n",
            "Epoch [5/5], Step [200/500], Loss: 3.2780\n",
            "Epoch [5/5], Step [300/500], Loss: 3.0819\n",
            "Epoch [5/5], Step [400/500], Loss: 2.9960\n",
            "Epoch [5/5], Step [500/500], Loss: 3.1401\n",
            "\n",
            "F16 precision:\n",
            "Total execution time = 60.602 sec\n",
            "Max memory used by tensors = 92842496 bytes\n",
            "Files already downloaded and verified\n",
            "Accuracy of the network on the 10000 test images: 23 %\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import argparse\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from torch import nn\n",
        "import gc\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('-n', '--nodes', default=1, type=int, metavar='N',\n",
        "                        help='number of data loading workers (default: 4)')\n",
        "    parser.add_argument('-g', '--gpus', default=1, type=int,\n",
        "                        help='number of gpus per node')\n",
        "    parser.add_argument('-nr', '--nr', default=0, type=int,\n",
        "                        help='ranking within the nodes')\n",
        "    parser.add_argument('--epochs', default=5, type=int, metavar='N',\n",
        "                        help='number of total epochs to run')\n",
        "    args = parser.parse_args([])\n",
        "    args.world_size = args.gpus * args.nodes\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '8888'\n",
        "\n",
        "    test(train(args, False, False), False) # Standard\n",
        "    test(train(args, True, False), False) # Mixed precision\n",
        "    test(train(args, False, True), True) # F16 everywhere\n",
        "\n",
        "\n",
        "def train(args, amp, f16):\n",
        "    torch.manual_seed(42)\n",
        "    model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)\n",
        "\n",
        "    if f16:\n",
        "      model = model.to(torch.float16)\n",
        "    else:\n",
        "      model = model.to(torch.float32)\n",
        "\n",
        "    model = model.cuda()\n",
        "    model.train()\n",
        "    batch_size = 100\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss().cuda()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), 1e-4)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
        "    train_dataset = torchvision.datasets.CIFAR10(root='./data',\n",
        "                                               train=True,\n",
        "                                               transform=transforms.ToTensor(),\n",
        "                                               download=True)\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=True,\n",
        "                                               num_workers=0,\n",
        "                                               pin_memory=True)\n",
        "\n",
        "    total_step = len(train_loader)\n",
        "    start_timer()\n",
        "    for epoch in range(args.epochs):\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            with torch.autocast(\"cuda\", dtype=torch.float16, enabled=(amp or f16)):\n",
        "              images = images.cuda(non_blocking=True)\n",
        "              labels = labels.cuda(non_blocking=True)\n",
        "              output = model(images)\n",
        "              loss = criterion(output, labels)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            if (i + 1) % batch_size == 0:\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, args.epochs, i + 1, total_step, loss.item()))\n",
        "\n",
        "    if f16:\n",
        "      end_timer_and_print(\"F16 precision:\")\n",
        "    elif amp:\n",
        "      end_timer_and_print(\"Mixed precision:\")\n",
        "    else:\n",
        "      end_timer_and_print(\"Standart precision:\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def test(model, f16, batch_size = 100):\n",
        "  model.eval()\n",
        "  if f16:\n",
        "    model = model.to(torch.float16)\n",
        "  else:\n",
        "    model = model.to(torch.float32)\n",
        "\n",
        "  test_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data',\n",
        "        train=False,\n",
        "        transform=transforms.ToTensor(),\n",
        "        download=True\n",
        "    )\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "      dataset=test_dataset,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False,\n",
        "      num_workers=0,\n",
        "      pin_memory=True,)\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "      for images, labels in test_loader:\n",
        "        with torch.autocast(\"cuda\", dtype=torch.float16, enabled=f16):\n",
        "          images = images.cuda(non_blocking=True)\n",
        "          labels = labels.cuda(non_blocking=True)\n",
        "          outputs = model(images)\n",
        "\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "  print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhJWHopK-4ty",
        "outputId": "02961361-7d32-48ad-9f81-ca506d207d50"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.10/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n",
            "  warnings.warn(msg, DeprecatedFeatureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Files already downloaded and verified\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Epoch [1/5], Step [100/500], Loss: 6.9227\n",
            "Epoch [1/5], Step [200/500], Loss: 6.6222\n",
            "Epoch [1/5], Step [300/500], Loss: 6.2470\n",
            "Epoch [1/5], Step [400/500], Loss: 5.9257\n",
            "Epoch [1/5], Step [500/500], Loss: 5.5271\n",
            "Epoch [2/5], Step [100/500], Loss: 5.1665\n",
            "Epoch [2/5], Step [200/500], Loss: 4.6391\n",
            "Epoch [2/5], Step [300/500], Loss: 4.2576\n",
            "Epoch [2/5], Step [400/500], Loss: 4.2456\n",
            "Epoch [2/5], Step [500/500], Loss: 3.8479\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "Epoch [3/5], Step [100/500], Loss: 3.5950\n",
            "Epoch [3/5], Step [200/500], Loss: 3.3599\n",
            "Epoch [3/5], Step [300/500], Loss: 3.1888\n",
            "Epoch [3/5], Step [400/500], Loss: 2.9637\n",
            "Epoch [3/5], Step [500/500], Loss: 2.8085\n",
            "Epoch [4/5], Step [100/500], Loss: 2.8607\n",
            "Epoch [4/5], Step [200/500], Loss: 2.4544\n",
            "Epoch [4/5], Step [300/500], Loss: 2.5778\n",
            "Epoch [4/5], Step [400/500], Loss: 2.4234\n",
            "Epoch [4/5], Step [500/500], Loss: 2.3195\n",
            "Epoch [5/5], Step [100/500], Loss: 2.1669\n",
            "Epoch [5/5], Step [200/500], Loss: 2.3844\n",
            "Epoch [5/5], Step [300/500], Loss: 2.1325\n",
            "Epoch [5/5], Step [400/500], Loss: 2.1359\n",
            "Epoch [5/5], Step [500/500], Loss: 2.1966\n",
            "\n",
            "Mixed precision:\n",
            "Total execution time = 64.218 sec\n",
            "Max memory used by tensors = 162331648 bytes\n",
            "Files already downloaded and verified\n",
            "Accuracy of the network on the 10000 test images: 32 %\n",
            "Selected optimization level O3:  Pure FP16 training.\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O3\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : False\n",
            "master_weights         : False\n",
            "loss_scale             : 1.0\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O3\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : False\n",
            "master_weights         : False\n",
            "loss_scale             : 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Epoch [1/5], Step [100/500], Loss: 7.0409\n",
            "Epoch [1/5], Step [200/500], Loss: 6.8294\n",
            "Epoch [1/5], Step [300/500], Loss: 6.5850\n",
            "Epoch [1/5], Step [400/500], Loss: 6.4001\n",
            "Epoch [1/5], Step [500/500], Loss: 6.1566\n",
            "Epoch [2/5], Step [100/500], Loss: 5.8349\n",
            "Epoch [2/5], Step [200/500], Loss: 5.5495\n",
            "Epoch [2/5], Step [300/500], Loss: 5.1978\n",
            "Epoch [2/5], Step [400/500], Loss: 5.1349\n",
            "Epoch [2/5], Step [500/500], Loss: 4.9295\n",
            "Epoch [3/5], Step [100/500], Loss: 4.7122\n",
            "Epoch [3/5], Step [200/500], Loss: 4.4653\n",
            "Epoch [3/5], Step [300/500], Loss: 4.2619\n",
            "Epoch [3/5], Step [400/500], Loss: 4.2164\n",
            "Epoch [3/5], Step [500/500], Loss: 3.9607\n",
            "Epoch [4/5], Step [100/500], Loss: 3.9320\n",
            "Epoch [4/5], Step [200/500], Loss: 3.5850\n",
            "Epoch [4/5], Step [300/500], Loss: 3.5993\n",
            "Epoch [4/5], Step [400/500], Loss: 3.4297\n",
            "Epoch [4/5], Step [500/500], Loss: 3.3723\n",
            "Epoch [5/5], Step [100/500], Loss: 3.3171\n",
            "Epoch [5/5], Step [200/500], Loss: 3.2811\n",
            "Epoch [5/5], Step [300/500], Loss: 3.0809\n",
            "Epoch [5/5], Step [400/500], Loss: 3.0359\n",
            "Epoch [5/5], Step [500/500], Loss: 3.1697\n",
            "\n",
            "F16 precision:\n",
            "Total execution time = 58.409 sec\n",
            "Max memory used by tensors = 99827712 bytes\n",
            "Files already downloaded and verified\n",
            "Accuracy of the network on the 10000 test images: 22 %\n"
          ]
        }
      ],
      "source": [
        "from apex import amp as amp_lib\n",
        "import os\n",
        "from datetime import datetime\n",
        "import argparse\n",
        "import torch.multiprocessing as mp\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.distributed as dist\n",
        "import apex\n",
        "import gc\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('-n', '--nodes', default=1, type=int, metavar='N',\n",
        "                        help='number of data loading workers (default: 4)')\n",
        "    parser.add_argument('-g', '--gpus', default=1, type=int,\n",
        "                        help='number of gpus per node')\n",
        "    parser.add_argument('-nr', '--nr', default=0, type=int,\n",
        "                        help='ranking within the nodes')\n",
        "    parser.add_argument('--epochs', default=5, type=int, metavar='N',\n",
        "                        help='number of total epochs to run')\n",
        "    args = parser.parse_args([])\n",
        "    args.world_size = args.gpus * args.nodes\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '8888'\n",
        "\n",
        "    test(train(args, True, False), False) # Mixed precision\n",
        "    test(train(args, False, True), True) # F16 everywhere\n",
        "\n",
        "\n",
        "def train(args, amp, f16):\n",
        "    torch.manual_seed(42)\n",
        "    model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)\n",
        "    model = model.to(torch.float32)\n",
        "    model = model.cuda()\n",
        "    model.train()\n",
        "    batch_size = 100\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss().cuda()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), 1e-4)\n",
        "    if amp:\n",
        "      opt_level = \"O2\"\n",
        "    elif f16:\n",
        "      opt_level = \"O3\"\n",
        "    model, optimizer = amp_lib.initialize(model, optimizer, opt_level=opt_level)\n",
        "    train_dataset = torchvision.datasets.CIFAR10(root='./data',\n",
        "                                               train=True,\n",
        "                                               transform=transforms.ToTensor(),\n",
        "                                               download=True)\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=True,\n",
        "                                               num_workers=0,\n",
        "                                               pin_memory=True)\n",
        "\n",
        "    total_step = len(train_loader)\n",
        "    start_timer()\n",
        "    for epoch in range(args.epochs):\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images = images.cuda(non_blocking=True)\n",
        "            labels = labels.cuda(non_blocking=True)\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            with amp_lib.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                scaled_loss.backward()\n",
        "            optimizer.step()\n",
        "            if (i + 1) % batch_size == 0:\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(\n",
        "                    epoch + 1,\n",
        "                    args.epochs,\n",
        "                    i + 1,\n",
        "                    total_step,\n",
        "                    loss.item()))\n",
        "\n",
        "    if f16:\n",
        "      end_timer_and_print(\"F16 precision:\")\n",
        "    elif amp:\n",
        "      end_timer_and_print(\"Mixed precision:\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def test(model, f16, batch_size = 100):\n",
        "  model.eval()\n",
        "  test_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data',\n",
        "        train=False,\n",
        "        transform=transforms.ToTensor(),\n",
        "        download=True\n",
        "    )\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "      dataset=test_dataset,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False,\n",
        "      num_workers=0,\n",
        "      pin_memory=True,)\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "      images = images.cuda(non_blocking=True)\n",
        "      labels = labels.cuda(non_blocking=True)\n",
        "      outputs = model(images)\n",
        "\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "  print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "usbBz1EbHq5G",
        "outputId": "8a4c19e7-f21d-4391-905e-f40296ecca42"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:329: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Step [32/1563], Loss: 7.2048\n",
            "Epoch [1/5], Step [64/1563], Loss: 7.0966\n",
            "Epoch [1/5], Step [96/1563], Loss: 7.2747\n",
            "Epoch [1/5], Step [128/1563], Loss: 6.8985\n",
            "Epoch [1/5], Step [160/1563], Loss: 6.7508\n",
            "Epoch [1/5], Step [192/1563], Loss: 6.4326\n",
            "Epoch [1/5], Step [224/1563], Loss: 6.5897\n",
            "Epoch [1/5], Step [256/1563], Loss: 6.4370\n",
            "Epoch [1/5], Step [288/1563], Loss: 6.2156\n",
            "Epoch [1/5], Step [320/1563], Loss: 6.1928\n",
            "Epoch [1/5], Step [352/1563], Loss: 5.9927\n",
            "Epoch [1/5], Step [384/1563], Loss: 5.8934\n",
            "Epoch [1/5], Step [416/1563], Loss: 5.5741\n",
            "Epoch [1/5], Step [448/1563], Loss: 5.4609\n",
            "Epoch [1/5], Step [480/1563], Loss: 5.6098\n",
            "Epoch [1/5], Step [512/1563], Loss: 5.5587\n",
            "Epoch [1/5], Step [544/1563], Loss: 5.4192\n",
            "Epoch [1/5], Step [576/1563], Loss: 4.9475\n",
            "Epoch [1/5], Step [608/1563], Loss: 4.9196\n",
            "Epoch [1/5], Step [640/1563], Loss: 4.9008\n",
            "Epoch [1/5], Step [672/1563], Loss: 5.0907\n",
            "Epoch [1/5], Step [704/1563], Loss: 4.5843\n",
            "Epoch [1/5], Step [736/1563], Loss: 4.2456\n",
            "Epoch [1/5], Step [768/1563], Loss: 4.4654\n",
            "Epoch [1/5], Step [800/1563], Loss: 4.1669\n",
            "Epoch [1/5], Step [832/1563], Loss: 4.4181\n",
            "Epoch [1/5], Step [864/1563], Loss: 4.1492\n",
            "Epoch [1/5], Step [896/1563], Loss: 3.9528\n",
            "Epoch [1/5], Step [928/1563], Loss: 4.2189\n",
            "Epoch [1/5], Step [960/1563], Loss: 4.1070\n",
            "Epoch [1/5], Step [992/1563], Loss: 3.9335\n",
            "Epoch [1/5], Step [1024/1563], Loss: 3.3757\n",
            "Epoch [1/5], Step [1056/1563], Loss: 3.6558\n",
            "Epoch [1/5], Step [1088/1563], Loss: 3.8054\n",
            "Epoch [1/5], Step [1120/1563], Loss: 3.4381\n",
            "Epoch [1/5], Step [1152/1563], Loss: 3.4888\n",
            "Epoch [1/5], Step [1184/1563], Loss: 2.9750\n",
            "Epoch [1/5], Step [1216/1563], Loss: 3.2156\n",
            "Epoch [1/5], Step [1248/1563], Loss: 3.1030\n",
            "Epoch [1/5], Step [1280/1563], Loss: 3.2625\n",
            "Epoch [1/5], Step [1312/1563], Loss: 3.0621\n",
            "Epoch [1/5], Step [1344/1563], Loss: 3.3138\n",
            "Epoch [1/5], Step [1376/1563], Loss: 3.0629\n",
            "Epoch [1/5], Step [1408/1563], Loss: 3.3472\n",
            "Epoch [1/5], Step [1440/1563], Loss: 2.9229\n",
            "Epoch [1/5], Step [1472/1563], Loss: 2.9672\n",
            "Epoch [1/5], Step [1504/1563], Loss: 2.7197\n",
            "Epoch [1/5], Step [1536/1563], Loss: 2.6485\n",
            "Epoch [2/5], Step [32/1563], Loss: 2.6428\n",
            "Epoch [2/5], Step [64/1563], Loss: 2.8287\n",
            "Epoch [2/5], Step [96/1563], Loss: 2.5914\n",
            "Epoch [2/5], Step [128/1563], Loss: 2.4076\n",
            "Epoch [2/5], Step [160/1563], Loss: 2.6821\n",
            "Epoch [2/5], Step [192/1563], Loss: 2.4402\n",
            "Epoch [2/5], Step [224/1563], Loss: 2.4588\n",
            "Epoch [2/5], Step [256/1563], Loss: 2.3949\n",
            "Epoch [2/5], Step [288/1563], Loss: 2.3460\n",
            "Epoch [2/5], Step [320/1563], Loss: 2.2466\n",
            "Epoch [2/5], Step [352/1563], Loss: 2.2969\n",
            "Epoch [2/5], Step [384/1563], Loss: 2.7256\n",
            "Epoch [2/5], Step [416/1563], Loss: 2.6423\n",
            "Epoch [2/5], Step [448/1563], Loss: 2.3991\n",
            "Epoch [2/5], Step [480/1563], Loss: 2.2840\n",
            "Epoch [2/5], Step [512/1563], Loss: 2.3221\n",
            "Epoch [2/5], Step [544/1563], Loss: 2.5382\n",
            "Epoch [2/5], Step [576/1563], Loss: 2.2651\n",
            "Epoch [2/5], Step [608/1563], Loss: 2.5664\n",
            "Epoch [2/5], Step [640/1563], Loss: 2.3467\n",
            "Epoch [2/5], Step [672/1563], Loss: 2.5404\n",
            "Epoch [2/5], Step [704/1563], Loss: 2.1521\n",
            "Epoch [2/5], Step [736/1563], Loss: 2.2556\n",
            "Epoch [2/5], Step [768/1563], Loss: 2.1848\n",
            "Epoch [2/5], Step [800/1563], Loss: 1.9496\n",
            "Epoch [2/5], Step [832/1563], Loss: 2.1889\n",
            "Epoch [2/5], Step [864/1563], Loss: 2.2828\n",
            "Epoch [2/5], Step [896/1563], Loss: 2.3033\n",
            "Epoch [2/5], Step [928/1563], Loss: 1.9363\n",
            "Epoch [2/5], Step [960/1563], Loss: 2.0837\n",
            "Epoch [2/5], Step [992/1563], Loss: 2.3243\n",
            "Epoch [2/5], Step [1024/1563], Loss: 2.2782\n",
            "Epoch [2/5], Step [1056/1563], Loss: 1.9033\n",
            "Epoch [2/5], Step [1088/1563], Loss: 2.1135\n",
            "Epoch [2/5], Step [1120/1563], Loss: 1.9318\n",
            "Epoch [2/5], Step [1152/1563], Loss: 1.9708\n",
            "Epoch [2/5], Step [1184/1563], Loss: 2.2831\n",
            "Epoch [2/5], Step [1216/1563], Loss: 2.1788\n",
            "Epoch [2/5], Step [1248/1563], Loss: 2.4646\n",
            "Epoch [2/5], Step [1280/1563], Loss: 2.1525\n",
            "Epoch [2/5], Step [1312/1563], Loss: 1.9160\n",
            "Epoch [2/5], Step [1344/1563], Loss: 2.3749\n",
            "Epoch [2/5], Step [1376/1563], Loss: 2.1466\n",
            "Epoch [2/5], Step [1408/1563], Loss: 1.9802\n",
            "Epoch [2/5], Step [1440/1563], Loss: 2.1519\n",
            "Epoch [2/5], Step [1472/1563], Loss: 2.1290\n",
            "Epoch [2/5], Step [1504/1563], Loss: 2.1394\n",
            "Epoch [2/5], Step [1536/1563], Loss: 2.0339\n",
            "Epoch [3/5], Step [32/1563], Loss: 1.9432\n",
            "Epoch [3/5], Step [64/1563], Loss: 2.0813\n",
            "Epoch [3/5], Step [96/1563], Loss: 1.8771\n",
            "Epoch [3/5], Step [128/1563], Loss: 2.1829\n",
            "Epoch [3/5], Step [160/1563], Loss: 2.0468\n",
            "Epoch [3/5], Step [192/1563], Loss: 2.0841\n",
            "Epoch [3/5], Step [224/1563], Loss: 1.9001\n",
            "Epoch [3/5], Step [256/1563], Loss: 1.8466\n",
            "Epoch [3/5], Step [288/1563], Loss: 1.8730\n",
            "Epoch [3/5], Step [320/1563], Loss: 2.1912\n",
            "Epoch [3/5], Step [352/1563], Loss: 2.1932\n",
            "Epoch [3/5], Step [384/1563], Loss: 1.6946\n",
            "Epoch [3/5], Step [416/1563], Loss: 2.2032\n",
            "Epoch [3/5], Step [448/1563], Loss: 1.8430\n",
            "Epoch [3/5], Step [480/1563], Loss: 2.0940\n",
            "Epoch [3/5], Step [512/1563], Loss: 1.8830\n",
            "Epoch [3/5], Step [544/1563], Loss: 1.9187\n",
            "Epoch [3/5], Step [576/1563], Loss: 1.6084\n",
            "Epoch [3/5], Step [608/1563], Loss: 2.0096\n",
            "Epoch [3/5], Step [640/1563], Loss: 1.8329\n",
            "Epoch [3/5], Step [672/1563], Loss: 1.8714\n",
            "Epoch [3/5], Step [704/1563], Loss: 2.0361\n",
            "Epoch [3/5], Step [736/1563], Loss: 1.7860\n",
            "Epoch [3/5], Step [768/1563], Loss: 1.8772\n",
            "Epoch [3/5], Step [800/1563], Loss: 1.9427\n",
            "Epoch [3/5], Step [832/1563], Loss: 2.0999\n",
            "Epoch [3/5], Step [864/1563], Loss: 1.7076\n",
            "Epoch [3/5], Step [896/1563], Loss: 1.8218\n",
            "Epoch [3/5], Step [928/1563], Loss: 1.7506\n",
            "Epoch [3/5], Step [960/1563], Loss: 1.6774\n",
            "Epoch [3/5], Step [992/1563], Loss: 2.0887\n",
            "Epoch [3/5], Step [1024/1563], Loss: 1.7142\n",
            "Epoch [3/5], Step [1056/1563], Loss: 1.9578\n",
            "Epoch [3/5], Step [1088/1563], Loss: 1.7061\n",
            "Epoch [3/5], Step [1120/1563], Loss: 1.7430\n",
            "Epoch [3/5], Step [1152/1563], Loss: 1.9705\n",
            "Epoch [3/5], Step [1184/1563], Loss: 1.8264\n",
            "Epoch [3/5], Step [1216/1563], Loss: 1.9572\n",
            "Epoch [3/5], Step [1248/1563], Loss: 1.9154\n",
            "Epoch [3/5], Step [1280/1563], Loss: 1.7936\n",
            "Epoch [3/5], Step [1312/1563], Loss: 1.7620\n",
            "Epoch [3/5], Step [1344/1563], Loss: 2.0820\n",
            "Epoch [3/5], Step [1376/1563], Loss: 1.5435\n",
            "Epoch [3/5], Step [1408/1563], Loss: 1.5998\n",
            "Epoch [3/5], Step [1440/1563], Loss: 2.0324\n",
            "Epoch [3/5], Step [1472/1563], Loss: 1.9718\n",
            "Epoch [3/5], Step [1504/1563], Loss: 1.9155\n",
            "Epoch [3/5], Step [1536/1563], Loss: 1.7987\n",
            "Epoch [4/5], Step [32/1563], Loss: 1.6126\n",
            "Epoch [4/5], Step [64/1563], Loss: 1.8566\n",
            "Epoch [4/5], Step [96/1563], Loss: 2.0287\n",
            "Epoch [4/5], Step [128/1563], Loss: 1.8204\n",
            "Epoch [4/5], Step [160/1563], Loss: 1.9527\n",
            "Epoch [4/5], Step [192/1563], Loss: 1.9541\n",
            "Epoch [4/5], Step [224/1563], Loss: 1.6513\n",
            "Epoch [4/5], Step [256/1563], Loss: 1.7754\n",
            "Epoch [4/5], Step [288/1563], Loss: 1.6388\n",
            "Epoch [4/5], Step [320/1563], Loss: 2.0959\n",
            "Epoch [4/5], Step [352/1563], Loss: 1.9302\n",
            "Epoch [4/5], Step [384/1563], Loss: 1.7893\n",
            "Epoch [4/5], Step [416/1563], Loss: 1.7377\n",
            "Epoch [4/5], Step [448/1563], Loss: 1.8934\n",
            "Epoch [4/5], Step [480/1563], Loss: 1.6430\n",
            "Epoch [4/5], Step [512/1563], Loss: 1.5881\n",
            "Epoch [4/5], Step [544/1563], Loss: 1.8373\n",
            "Epoch [4/5], Step [576/1563], Loss: 1.7546\n",
            "Epoch [4/5], Step [608/1563], Loss: 2.0222\n",
            "Epoch [4/5], Step [640/1563], Loss: 1.6582\n",
            "Epoch [4/5], Step [672/1563], Loss: 2.4192\n",
            "Epoch [4/5], Step [704/1563], Loss: 1.6380\n",
            "Epoch [4/5], Step [736/1563], Loss: 1.4785\n",
            "Epoch [4/5], Step [768/1563], Loss: 1.7500\n",
            "Epoch [4/5], Step [800/1563], Loss: 1.8003\n",
            "Epoch [4/5], Step [832/1563], Loss: 1.6427\n",
            "Epoch [4/5], Step [864/1563], Loss: 1.6552\n",
            "Epoch [4/5], Step [896/1563], Loss: 1.6933\n",
            "Epoch [4/5], Step [928/1563], Loss: 2.0385\n",
            "Epoch [4/5], Step [960/1563], Loss: 1.7158\n",
            "Epoch [4/5], Step [992/1563], Loss: 1.7852\n",
            "Epoch [4/5], Step [1024/1563], Loss: 1.7294\n",
            "Epoch [4/5], Step [1056/1563], Loss: 2.0028\n",
            "Epoch [4/5], Step [1088/1563], Loss: 2.0563\n",
            "Epoch [4/5], Step [1120/1563], Loss: 1.8740\n",
            "Epoch [4/5], Step [1152/1563], Loss: 1.8705\n",
            "Epoch [4/5], Step [1184/1563], Loss: 1.7374\n",
            "Epoch [4/5], Step [1216/1563], Loss: 1.7671\n",
            "Epoch [4/5], Step [1248/1563], Loss: 1.6564\n",
            "Epoch [4/5], Step [1280/1563], Loss: 2.1787\n",
            "Epoch [4/5], Step [1312/1563], Loss: 1.6676\n",
            "Epoch [4/5], Step [1344/1563], Loss: 1.6224\n",
            "Epoch [4/5], Step [1376/1563], Loss: 1.7240\n",
            "Epoch [4/5], Step [1408/1563], Loss: 1.4755\n",
            "Epoch [4/5], Step [1440/1563], Loss: 1.8415\n",
            "Epoch [4/5], Step [1472/1563], Loss: 1.7049\n",
            "Epoch [4/5], Step [1504/1563], Loss: 1.3789\n",
            "Epoch [4/5], Step [1536/1563], Loss: 1.7698\n",
            "Epoch [5/5], Step [32/1563], Loss: 1.8834\n",
            "Epoch [5/5], Step [64/1563], Loss: 1.3731\n",
            "Epoch [5/5], Step [96/1563], Loss: 1.6954\n",
            "Epoch [5/5], Step [128/1563], Loss: 1.7792\n",
            "Epoch [5/5], Step [160/1563], Loss: 1.6179\n",
            "Epoch [5/5], Step [192/1563], Loss: 1.5310\n",
            "Epoch [5/5], Step [224/1563], Loss: 1.9116\n",
            "Epoch [5/5], Step [256/1563], Loss: 1.4037\n",
            "Epoch [5/5], Step [288/1563], Loss: 1.5800\n",
            "Epoch [5/5], Step [320/1563], Loss: 1.5635\n",
            "Epoch [5/5], Step [352/1563], Loss: 1.5311\n",
            "Epoch [5/5], Step [384/1563], Loss: 1.7544\n",
            "Epoch [5/5], Step [416/1563], Loss: 1.4997\n",
            "Epoch [5/5], Step [448/1563], Loss: 1.5461\n",
            "Epoch [5/5], Step [480/1563], Loss: 1.9715\n",
            "Epoch [5/5], Step [512/1563], Loss: 1.5817\n",
            "Epoch [5/5], Step [544/1563], Loss: 1.4644\n",
            "Epoch [5/5], Step [576/1563], Loss: 1.6186\n",
            "Epoch [5/5], Step [608/1563], Loss: 1.9966\n",
            "Epoch [5/5], Step [640/1563], Loss: 1.5891\n",
            "Epoch [5/5], Step [672/1563], Loss: 1.8057\n",
            "Epoch [5/5], Step [704/1563], Loss: 1.8309\n",
            "Epoch [5/5], Step [736/1563], Loss: 1.8370\n",
            "Epoch [5/5], Step [768/1563], Loss: 1.6275\n",
            "Epoch [5/5], Step [800/1563], Loss: 1.6259\n",
            "Epoch [5/5], Step [832/1563], Loss: 1.6365\n",
            "Epoch [5/5], Step [864/1563], Loss: 1.9661\n",
            "Epoch [5/5], Step [896/1563], Loss: 1.3394\n",
            "Epoch [5/5], Step [928/1563], Loss: 1.7222\n",
            "Epoch [5/5], Step [960/1563], Loss: 1.7903\n",
            "Epoch [5/5], Step [992/1563], Loss: 1.8341\n",
            "Epoch [5/5], Step [1024/1563], Loss: 1.8416\n",
            "Epoch [5/5], Step [1056/1563], Loss: 1.6245\n",
            "Epoch [5/5], Step [1088/1563], Loss: 1.5285\n",
            "Epoch [5/5], Step [1120/1563], Loss: 1.6002\n",
            "Epoch [5/5], Step [1152/1563], Loss: 1.3698\n",
            "Epoch [5/5], Step [1184/1563], Loss: 1.4872\n",
            "Epoch [5/5], Step [1216/1563], Loss: 1.7321\n",
            "Epoch [5/5], Step [1248/1563], Loss: 1.4851\n",
            "Epoch [5/5], Step [1280/1563], Loss: 1.4269\n",
            "Epoch [5/5], Step [1312/1563], Loss: 1.5223\n",
            "Epoch [5/5], Step [1344/1563], Loss: 1.6238\n",
            "Epoch [5/5], Step [1376/1563], Loss: 1.9516\n",
            "Epoch [5/5], Step [1408/1563], Loss: 1.3981\n",
            "Epoch [5/5], Step [1440/1563], Loss: 1.5605\n",
            "Epoch [5/5], Step [1472/1563], Loss: 2.0333\n",
            "Epoch [5/5], Step [1504/1563], Loss: 1.7272\n",
            "Epoch [5/5], Step [1536/1563], Loss: 1.6875\n",
            "\n",
            "Standart precision:\n",
            "Total execution time = 106.870 sec\n",
            "Max memory used by tensors = 190054912 bytes\n",
            "Files already downloaded and verified\n",
            "Accuracy of the network on the 10000 test images: 41 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Epoch [1/5], Step [32/1563], Loss: 7.2458\n",
            "Epoch [1/5], Step [64/1563], Loss: 7.1433\n",
            "Epoch [1/5], Step [96/1563], Loss: 7.2489\n",
            "Epoch [1/5], Step [128/1563], Loss: 6.9563\n",
            "Epoch [1/5], Step [160/1563], Loss: 6.8020\n",
            "Epoch [1/5], Step [192/1563], Loss: 6.5088\n",
            "Epoch [1/5], Step [224/1563], Loss: 6.6088\n",
            "Epoch [1/5], Step [256/1563], Loss: 6.3727\n",
            "Epoch [1/5], Step [288/1563], Loss: 6.2603\n",
            "Epoch [1/5], Step [320/1563], Loss: 6.0604\n",
            "Epoch [1/5], Step [352/1563], Loss: 6.1151\n",
            "Epoch [1/5], Step [384/1563], Loss: 5.9714\n",
            "Epoch [1/5], Step [416/1563], Loss: 5.5478\n",
            "Epoch [1/5], Step [448/1563], Loss: 5.4944\n",
            "Epoch [1/5], Step [480/1563], Loss: 5.5835\n",
            "Epoch [1/5], Step [512/1563], Loss: 5.5022\n",
            "Epoch [1/5], Step [544/1563], Loss: 5.4563\n",
            "Epoch [1/5], Step [576/1563], Loss: 4.8080\n",
            "Epoch [1/5], Step [608/1563], Loss: 5.0001\n",
            "Epoch [1/5], Step [640/1563], Loss: 4.9603\n",
            "Epoch [1/5], Step [672/1563], Loss: 5.0196\n",
            "Epoch [1/5], Step [704/1563], Loss: 4.8317\n",
            "Epoch [1/5], Step [736/1563], Loss: 4.3702\n",
            "Epoch [1/5], Step [768/1563], Loss: 4.4237\n",
            "Epoch [1/5], Step [800/1563], Loss: 4.1878\n",
            "Epoch [1/5], Step [832/1563], Loss: 4.3090\n",
            "Epoch [1/5], Step [864/1563], Loss: 4.3890\n",
            "Epoch [1/5], Step [896/1563], Loss: 3.8633\n",
            "Epoch [1/5], Step [928/1563], Loss: 4.0181\n",
            "Epoch [1/5], Step [960/1563], Loss: 4.0232\n",
            "Epoch [1/5], Step [992/1563], Loss: 4.0368\n",
            "Epoch [1/5], Step [1024/1563], Loss: 3.4328\n",
            "Epoch [1/5], Step [1056/1563], Loss: 3.7025\n",
            "Epoch [1/5], Step [1088/1563], Loss: 3.7144\n",
            "Epoch [1/5], Step [1120/1563], Loss: 3.5117\n",
            "Epoch [1/5], Step [1152/1563], Loss: 3.4022\n",
            "Epoch [1/5], Step [1184/1563], Loss: 3.0048\n",
            "Epoch [1/5], Step [1216/1563], Loss: 3.3829\n",
            "Epoch [1/5], Step [1248/1563], Loss: 3.1873\n",
            "Epoch [1/5], Step [1280/1563], Loss: 3.2682\n",
            "Epoch [1/5], Step [1312/1563], Loss: 3.0292\n",
            "Epoch [1/5], Step [1344/1563], Loss: 3.2990\n",
            "Epoch [1/5], Step [1376/1563], Loss: 3.0257\n",
            "Epoch [1/5], Step [1408/1563], Loss: 3.2058\n",
            "Epoch [1/5], Step [1440/1563], Loss: 2.7460\n",
            "Epoch [1/5], Step [1472/1563], Loss: 2.8802\n",
            "Epoch [1/5], Step [1504/1563], Loss: 2.8114\n",
            "Epoch [1/5], Step [1536/1563], Loss: 2.6116\n",
            "Epoch [2/5], Step [32/1563], Loss: 2.6587\n",
            "Epoch [2/5], Step [64/1563], Loss: 2.9294\n",
            "Epoch [2/5], Step [96/1563], Loss: 2.6856\n",
            "Epoch [2/5], Step [128/1563], Loss: 2.4280\n",
            "Epoch [2/5], Step [160/1563], Loss: 2.6643\n",
            "Epoch [2/5], Step [192/1563], Loss: 2.4119\n",
            "Epoch [2/5], Step [224/1563], Loss: 2.4319\n",
            "Epoch [2/5], Step [256/1563], Loss: 2.4828\n",
            "Epoch [2/5], Step [288/1563], Loss: 2.3365\n",
            "Epoch [2/5], Step [320/1563], Loss: 2.4102\n",
            "Epoch [2/5], Step [352/1563], Loss: 2.1328\n",
            "Epoch [2/5], Step [384/1563], Loss: 2.6129\n",
            "Epoch [2/5], Step [416/1563], Loss: 2.6471\n",
            "Epoch [2/5], Step [448/1563], Loss: 2.4007\n",
            "Epoch [2/5], Step [480/1563], Loss: 2.3309\n",
            "Epoch [2/5], Step [512/1563], Loss: 2.4025\n",
            "Epoch [2/5], Step [544/1563], Loss: 2.3920\n",
            "Epoch [2/5], Step [576/1563], Loss: 2.2771\n",
            "Epoch [2/5], Step [608/1563], Loss: 2.4108\n",
            "Epoch [2/5], Step [640/1563], Loss: 2.2705\n",
            "Epoch [2/5], Step [672/1563], Loss: 2.3476\n",
            "Epoch [2/5], Step [704/1563], Loss: 2.0628\n",
            "Epoch [2/5], Step [736/1563], Loss: 2.2724\n",
            "Epoch [2/5], Step [768/1563], Loss: 2.3476\n",
            "Epoch [2/5], Step [800/1563], Loss: 1.7675\n",
            "Epoch [2/5], Step [832/1563], Loss: 2.1681\n",
            "Epoch [2/5], Step [864/1563], Loss: 2.2843\n",
            "Epoch [2/5], Step [896/1563], Loss: 2.4756\n",
            "Epoch [2/5], Step [928/1563], Loss: 1.9177\n",
            "Epoch [2/5], Step [960/1563], Loss: 2.2937\n",
            "Epoch [2/5], Step [992/1563], Loss: 2.3297\n",
            "Epoch [2/5], Step [1024/1563], Loss: 2.2913\n",
            "Epoch [2/5], Step [1056/1563], Loss: 1.8796\n",
            "Epoch [2/5], Step [1088/1563], Loss: 1.8989\n",
            "Epoch [2/5], Step [1120/1563], Loss: 2.0031\n",
            "Epoch [2/5], Step [1152/1563], Loss: 2.0311\n",
            "Epoch [2/5], Step [1184/1563], Loss: 2.3526\n",
            "Epoch [2/5], Step [1216/1563], Loss: 2.3341\n",
            "Epoch [2/5], Step [1248/1563], Loss: 2.2690\n",
            "Epoch [2/5], Step [1280/1563], Loss: 1.9481\n",
            "Epoch [2/5], Step [1312/1563], Loss: 1.9826\n",
            "Epoch [2/5], Step [1344/1563], Loss: 2.3529\n",
            "Epoch [2/5], Step [1376/1563], Loss: 2.1837\n",
            "Epoch [2/5], Step [1408/1563], Loss: 1.9852\n",
            "Epoch [2/5], Step [1440/1563], Loss: 2.2780\n",
            "Epoch [2/5], Step [1472/1563], Loss: 2.1667\n",
            "Epoch [2/5], Step [1504/1563], Loss: 2.1296\n",
            "Epoch [2/5], Step [1536/1563], Loss: 2.0714\n",
            "Epoch [3/5], Step [32/1563], Loss: 2.0171\n",
            "Epoch [3/5], Step [64/1563], Loss: 1.9809\n",
            "Epoch [3/5], Step [96/1563], Loss: 2.0080\n",
            "Epoch [3/5], Step [128/1563], Loss: 2.0924\n",
            "Epoch [3/5], Step [160/1563], Loss: 1.9460\n",
            "Epoch [3/5], Step [192/1563], Loss: 2.1141\n",
            "Epoch [3/5], Step [224/1563], Loss: 1.8925\n",
            "Epoch [3/5], Step [256/1563], Loss: 1.8543\n",
            "Epoch [3/5], Step [288/1563], Loss: 1.7307\n",
            "Epoch [3/5], Step [320/1563], Loss: 2.2348\n",
            "Epoch [3/5], Step [352/1563], Loss: 2.0517\n",
            "Epoch [3/5], Step [384/1563], Loss: 1.7263\n",
            "Epoch [3/5], Step [416/1563], Loss: 2.1670\n",
            "Epoch [3/5], Step [448/1563], Loss: 1.7320\n",
            "Epoch [3/5], Step [480/1563], Loss: 2.1650\n",
            "Epoch [3/5], Step [512/1563], Loss: 1.9720\n",
            "Epoch [3/5], Step [544/1563], Loss: 1.9243\n",
            "Epoch [3/5], Step [576/1563], Loss: 1.7479\n",
            "Epoch [3/5], Step [608/1563], Loss: 1.8820\n",
            "Epoch [3/5], Step [640/1563], Loss: 1.8184\n",
            "Epoch [3/5], Step [672/1563], Loss: 1.9741\n",
            "Epoch [3/5], Step [704/1563], Loss: 2.3840\n",
            "Epoch [3/5], Step [736/1563], Loss: 1.8395\n",
            "Epoch [3/5], Step [768/1563], Loss: 1.8689\n",
            "Epoch [3/5], Step [800/1563], Loss: 1.9124\n",
            "Epoch [3/5], Step [832/1563], Loss: 2.1887\n",
            "Epoch [3/5], Step [864/1563], Loss: 1.6280\n",
            "Epoch [3/5], Step [896/1563], Loss: 1.6658\n",
            "Epoch [3/5], Step [928/1563], Loss: 1.8028\n",
            "Epoch [3/5], Step [960/1563], Loss: 1.8165\n",
            "Epoch [3/5], Step [992/1563], Loss: 2.0547\n",
            "Epoch [3/5], Step [1024/1563], Loss: 1.7857\n",
            "Epoch [3/5], Step [1056/1563], Loss: 1.8722\n",
            "Epoch [3/5], Step [1088/1563], Loss: 1.7952\n",
            "Epoch [3/5], Step [1120/1563], Loss: 1.6792\n",
            "Epoch [3/5], Step [1152/1563], Loss: 2.1157\n",
            "Epoch [3/5], Step [1184/1563], Loss: 1.8061\n",
            "Epoch [3/5], Step [1216/1563], Loss: 1.9482\n",
            "Epoch [3/5], Step [1248/1563], Loss: 1.8222\n",
            "Epoch [3/5], Step [1280/1563], Loss: 1.9032\n",
            "Epoch [3/5], Step [1312/1563], Loss: 1.6776\n",
            "Epoch [3/5], Step [1344/1563], Loss: 1.9911\n",
            "Epoch [3/5], Step [1376/1563], Loss: 1.6248\n",
            "Epoch [3/5], Step [1408/1563], Loss: 1.5666\n",
            "Epoch [3/5], Step [1440/1563], Loss: 1.9643\n",
            "Epoch [3/5], Step [1472/1563], Loss: 1.8784\n",
            "Epoch [3/5], Step [1504/1563], Loss: 1.8797\n",
            "Epoch [3/5], Step [1536/1563], Loss: 1.7612\n",
            "Epoch [4/5], Step [32/1563], Loss: 1.8123\n",
            "Epoch [4/5], Step [64/1563], Loss: 1.9443\n",
            "Epoch [4/5], Step [96/1563], Loss: 2.1758\n",
            "Epoch [4/5], Step [128/1563], Loss: 1.8831\n",
            "Epoch [4/5], Step [160/1563], Loss: 1.9368\n",
            "Epoch [4/5], Step [192/1563], Loss: 1.9068\n",
            "Epoch [4/5], Step [224/1563], Loss: 1.7094\n",
            "Epoch [4/5], Step [256/1563], Loss: 1.5621\n",
            "Epoch [4/5], Step [288/1563], Loss: 1.7204\n",
            "Epoch [4/5], Step [320/1563], Loss: 2.1114\n",
            "Epoch [4/5], Step [352/1563], Loss: 1.8492\n",
            "Epoch [4/5], Step [384/1563], Loss: 1.7658\n",
            "Epoch [4/5], Step [416/1563], Loss: 2.0020\n",
            "Epoch [4/5], Step [448/1563], Loss: 1.9165\n",
            "Epoch [4/5], Step [480/1563], Loss: 1.7380\n",
            "Epoch [4/5], Step [512/1563], Loss: 1.6490\n",
            "Epoch [4/5], Step [544/1563], Loss: 1.7814\n",
            "Epoch [4/5], Step [576/1563], Loss: 1.8767\n",
            "Epoch [4/5], Step [608/1563], Loss: 1.9536\n",
            "Epoch [4/5], Step [640/1563], Loss: 1.6395\n",
            "Epoch [4/5], Step [672/1563], Loss: 2.0524\n",
            "Epoch [4/5], Step [704/1563], Loss: 1.8212\n",
            "Epoch [4/5], Step [736/1563], Loss: 1.3888\n",
            "Epoch [4/5], Step [768/1563], Loss: 1.6307\n",
            "Epoch [4/5], Step [800/1563], Loss: 1.8631\n",
            "Epoch [4/5], Step [832/1563], Loss: 1.7463\n",
            "Epoch [4/5], Step [864/1563], Loss: 1.6252\n",
            "Epoch [4/5], Step [896/1563], Loss: 1.5867\n",
            "Epoch [4/5], Step [928/1563], Loss: 2.1483\n",
            "Epoch [4/5], Step [960/1563], Loss: 1.7571\n",
            "Epoch [4/5], Step [992/1563], Loss: 1.9469\n",
            "Epoch [4/5], Step [1024/1563], Loss: 1.7391\n",
            "Epoch [4/5], Step [1056/1563], Loss: 1.9470\n",
            "Epoch [4/5], Step [1088/1563], Loss: 1.9728\n",
            "Epoch [4/5], Step [1120/1563], Loss: 1.7189\n",
            "Epoch [4/5], Step [1152/1563], Loss: 1.8335\n",
            "Epoch [4/5], Step [1184/1563], Loss: 1.7036\n",
            "Epoch [4/5], Step [1216/1563], Loss: 1.6060\n",
            "Epoch [4/5], Step [1248/1563], Loss: 1.6640\n",
            "Epoch [4/5], Step [1280/1563], Loss: 2.1318\n",
            "Epoch [4/5], Step [1312/1563], Loss: 1.8901\n",
            "Epoch [4/5], Step [1344/1563], Loss: 1.7007\n",
            "Epoch [4/5], Step [1376/1563], Loss: 1.5660\n",
            "Epoch [4/5], Step [1408/1563], Loss: 1.4162\n",
            "Epoch [4/5], Step [1440/1563], Loss: 1.7790\n",
            "Epoch [4/5], Step [1472/1563], Loss: 1.5862\n",
            "Epoch [4/5], Step [1504/1563], Loss: 1.4101\n",
            "Epoch [4/5], Step [1536/1563], Loss: 1.7755\n",
            "Epoch [5/5], Step [32/1563], Loss: 1.7448\n",
            "Epoch [5/5], Step [64/1563], Loss: 1.5298\n",
            "Epoch [5/5], Step [96/1563], Loss: 1.7373\n",
            "Epoch [5/5], Step [128/1563], Loss: 1.9692\n",
            "Epoch [5/5], Step [160/1563], Loss: 1.4646\n",
            "Epoch [5/5], Step [192/1563], Loss: 1.5797\n",
            "Epoch [5/5], Step [224/1563], Loss: 2.0541\n",
            "Epoch [5/5], Step [256/1563], Loss: 1.3517\n",
            "Epoch [5/5], Step [288/1563], Loss: 1.6151\n",
            "Epoch [5/5], Step [320/1563], Loss: 1.6678\n",
            "Epoch [5/5], Step [352/1563], Loss: 1.6564\n",
            "Epoch [5/5], Step [384/1563], Loss: 1.6618\n",
            "Epoch [5/5], Step [416/1563], Loss: 1.5318\n",
            "Epoch [5/5], Step [448/1563], Loss: 1.7604\n",
            "Epoch [5/5], Step [480/1563], Loss: 1.8465\n",
            "Epoch [5/5], Step [512/1563], Loss: 1.5351\n",
            "Epoch [5/5], Step [544/1563], Loss: 1.4419\n",
            "Epoch [5/5], Step [576/1563], Loss: 1.6926\n",
            "Epoch [5/5], Step [608/1563], Loss: 1.9996\n",
            "Epoch [5/5], Step [640/1563], Loss: 1.5914\n",
            "Epoch [5/5], Step [672/1563], Loss: 1.7195\n",
            "Epoch [5/5], Step [704/1563], Loss: 1.7481\n",
            "Epoch [5/5], Step [736/1563], Loss: 1.8090\n",
            "Epoch [5/5], Step [768/1563], Loss: 1.6224\n",
            "Epoch [5/5], Step [800/1563], Loss: 1.6215\n",
            "Epoch [5/5], Step [832/1563], Loss: 1.7282\n",
            "Epoch [5/5], Step [864/1563], Loss: 1.8382\n",
            "Epoch [5/5], Step [896/1563], Loss: 1.2605\n",
            "Epoch [5/5], Step [928/1563], Loss: 1.5985\n",
            "Epoch [5/5], Step [960/1563], Loss: 1.8442\n",
            "Epoch [5/5], Step [992/1563], Loss: 1.8095\n",
            "Epoch [5/5], Step [1024/1563], Loss: 1.6477\n",
            "Epoch [5/5], Step [1056/1563], Loss: 1.6129\n",
            "Epoch [5/5], Step [1088/1563], Loss: 1.5894\n",
            "Epoch [5/5], Step [1120/1563], Loss: 1.6707\n",
            "Epoch [5/5], Step [1152/1563], Loss: 1.4254\n",
            "Epoch [5/5], Step [1184/1563], Loss: 1.4541\n",
            "Epoch [5/5], Step [1216/1563], Loss: 1.7497\n",
            "Epoch [5/5], Step [1248/1563], Loss: 1.5938\n",
            "Epoch [5/5], Step [1280/1563], Loss: 1.4013\n",
            "Epoch [5/5], Step [1312/1563], Loss: 1.5489\n",
            "Epoch [5/5], Step [1344/1563], Loss: 1.5426\n",
            "Epoch [5/5], Step [1376/1563], Loss: 2.0502\n",
            "Epoch [5/5], Step [1408/1563], Loss: 1.4680\n",
            "Epoch [5/5], Step [1440/1563], Loss: 1.6186\n",
            "Epoch [5/5], Step [1472/1563], Loss: 1.9855\n",
            "Epoch [5/5], Step [1504/1563], Loss: 1.7575\n",
            "Epoch [5/5], Step [1536/1563], Loss: 1.5901\n",
            "\n",
            "Mixed precision:\n",
            "Total execution time = 123.817 sec\n",
            "Max memory used by tensors = 167548928 bytes\n",
            "Files already downloaded and verified\n",
            "Accuracy of the network on the 10000 test images: 42 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Epoch [1/5], Step [32/1563], Loss: 7.2729\n",
            "Epoch [1/5], Step [64/1563], Loss: 7.2175\n",
            "Epoch [1/5], Step [96/1563], Loss: 7.3866\n",
            "Epoch [1/5], Step [128/1563], Loss: 7.0946\n",
            "Epoch [1/5], Step [160/1563], Loss: 6.9354\n",
            "Epoch [1/5], Step [192/1563], Loss: 6.6085\n",
            "Epoch [1/5], Step [224/1563], Loss: 6.7767\n",
            "Epoch [1/5], Step [256/1563], Loss: 6.5791\n",
            "Epoch [1/5], Step [288/1563], Loss: 6.4921\n",
            "Epoch [1/5], Step [320/1563], Loss: 6.4249\n",
            "Epoch [1/5], Step [352/1563], Loss: 6.4243\n",
            "Epoch [1/5], Step [384/1563], Loss: 6.2076\n",
            "Epoch [1/5], Step [416/1563], Loss: 5.8491\n",
            "Epoch [1/5], Step [448/1563], Loss: 5.9547\n",
            "Epoch [1/5], Step [480/1563], Loss: 5.9396\n",
            "Epoch [1/5], Step [512/1563], Loss: 5.9614\n",
            "Epoch [1/5], Step [544/1563], Loss: 5.9929\n",
            "Epoch [1/5], Step [576/1563], Loss: 5.3516\n",
            "Epoch [1/5], Step [608/1563], Loss: 5.3922\n",
            "Epoch [1/5], Step [640/1563], Loss: 5.3430\n",
            "Epoch [1/5], Step [672/1563], Loss: 5.5290\n",
            "Epoch [1/5], Step [704/1563], Loss: 5.3318\n",
            "Epoch [1/5], Step [736/1563], Loss: 5.1378\n",
            "Epoch [1/5], Step [768/1563], Loss: 4.9474\n",
            "Epoch [1/5], Step [800/1563], Loss: 4.8727\n",
            "Epoch [1/5], Step [832/1563], Loss: 4.9326\n",
            "Epoch [1/5], Step [864/1563], Loss: 4.9047\n",
            "Epoch [1/5], Step [896/1563], Loss: 4.5969\n",
            "Epoch [1/5], Step [928/1563], Loss: 4.7074\n",
            "Epoch [1/5], Step [960/1563], Loss: 4.8009\n",
            "Epoch [1/5], Step [992/1563], Loss: 4.6950\n",
            "Epoch [1/5], Step [1024/1563], Loss: 4.1168\n",
            "Epoch [1/5], Step [1056/1563], Loss: 4.4274\n",
            "Epoch [1/5], Step [1088/1563], Loss: 4.2919\n",
            "Epoch [1/5], Step [1120/1563], Loss: 4.2762\n",
            "Epoch [1/5], Step [1152/1563], Loss: 4.1940\n",
            "Epoch [1/5], Step [1184/1563], Loss: 3.8501\n",
            "Epoch [1/5], Step [1216/1563], Loss: 4.0458\n",
            "Epoch [1/5], Step [1248/1563], Loss: 4.0074\n",
            "Epoch [1/5], Step [1280/1563], Loss: 3.8090\n",
            "Epoch [1/5], Step [1312/1563], Loss: 3.8746\n",
            "Epoch [1/5], Step [1344/1563], Loss: 3.9855\n",
            "Epoch [1/5], Step [1376/1563], Loss: 3.7430\n",
            "Epoch [1/5], Step [1408/1563], Loss: 3.9079\n",
            "Epoch [1/5], Step [1440/1563], Loss: 3.5764\n",
            "Epoch [1/5], Step [1472/1563], Loss: 3.6366\n",
            "Epoch [1/5], Step [1504/1563], Loss: 3.4138\n",
            "Epoch [1/5], Step [1536/1563], Loss: 3.3445\n",
            "Epoch [2/5], Step [32/1563], Loss: 3.3009\n",
            "Epoch [2/5], Step [64/1563], Loss: 3.3928\n",
            "Epoch [2/5], Step [96/1563], Loss: 3.1685\n",
            "Epoch [2/5], Step [128/1563], Loss: 2.9435\n",
            "Epoch [2/5], Step [160/1563], Loss: 3.3305\n",
            "Epoch [2/5], Step [192/1563], Loss: 3.0911\n",
            "Epoch [2/5], Step [224/1563], Loss: 3.0175\n",
            "Epoch [2/5], Step [256/1563], Loss: 3.0339\n",
            "Epoch [2/5], Step [288/1563], Loss: 3.0463\n",
            "Epoch [2/5], Step [320/1563], Loss: 2.9214\n",
            "Epoch [2/5], Step [352/1563], Loss: 2.8788\n",
            "Epoch [2/5], Step [384/1563], Loss: 3.0662\n",
            "Epoch [2/5], Step [416/1563], Loss: 3.0640\n",
            "Epoch [2/5], Step [448/1563], Loss: 2.8950\n",
            "Epoch [2/5], Step [480/1563], Loss: 2.6465\n",
            "Epoch [2/5], Step [512/1563], Loss: 2.8733\n",
            "Epoch [2/5], Step [544/1563], Loss: 2.9432\n",
            "Epoch [2/5], Step [576/1563], Loss: 2.6118\n",
            "Epoch [2/5], Step [608/1563], Loss: 2.7854\n",
            "Epoch [2/5], Step [640/1563], Loss: 2.6935\n",
            "Epoch [2/5], Step [672/1563], Loss: 2.8913\n",
            "Epoch [2/5], Step [704/1563], Loss: 2.6266\n",
            "Epoch [2/5], Step [736/1563], Loss: 2.8005\n",
            "Epoch [2/5], Step [768/1563], Loss: 2.6205\n",
            "Epoch [2/5], Step [800/1563], Loss: 2.4354\n",
            "Epoch [2/5], Step [832/1563], Loss: 2.5355\n",
            "Epoch [2/5], Step [864/1563], Loss: 2.5102\n",
            "Epoch [2/5], Step [896/1563], Loss: 2.7291\n",
            "Epoch [2/5], Step [928/1563], Loss: 2.4173\n",
            "Epoch [2/5], Step [960/1563], Loss: 2.5157\n",
            "Epoch [2/5], Step [992/1563], Loss: 2.4643\n",
            "Epoch [2/5], Step [1024/1563], Loss: 2.5349\n",
            "Epoch [2/5], Step [1056/1563], Loss: 2.3142\n",
            "Epoch [2/5], Step [1088/1563], Loss: 2.4160\n",
            "Epoch [2/5], Step [1120/1563], Loss: 2.3690\n",
            "Epoch [2/5], Step [1152/1563], Loss: 2.3345\n",
            "Epoch [2/5], Step [1184/1563], Loss: 2.5630\n",
            "Epoch [2/5], Step [1216/1563], Loss: 2.6705\n",
            "Epoch [2/5], Step [1248/1563], Loss: 2.6960\n",
            "Epoch [2/5], Step [1280/1563], Loss: 2.4252\n",
            "Epoch [2/5], Step [1312/1563], Loss: 2.2274\n",
            "Epoch [2/5], Step [1344/1563], Loss: 2.5468\n",
            "Epoch [2/5], Step [1376/1563], Loss: 2.3969\n",
            "Epoch [2/5], Step [1408/1563], Loss: 2.3477\n",
            "Epoch [2/5], Step [1440/1563], Loss: 2.3877\n",
            "Epoch [2/5], Step [1472/1563], Loss: 2.4360\n",
            "Epoch [2/5], Step [1504/1563], Loss: 2.4382\n",
            "Epoch [2/5], Step [1536/1563], Loss: 2.4106\n",
            "Epoch [3/5], Step [32/1563], Loss: 2.2258\n",
            "Epoch [3/5], Step [64/1563], Loss: 2.3727\n",
            "Epoch [3/5], Step [96/1563], Loss: 2.3906\n",
            "Epoch [3/5], Step [128/1563], Loss: 2.4795\n",
            "Epoch [3/5], Step [160/1563], Loss: 2.2987\n",
            "Epoch [3/5], Step [192/1563], Loss: 2.3246\n",
            "Epoch [3/5], Step [224/1563], Loss: 2.1555\n",
            "Epoch [3/5], Step [256/1563], Loss: 2.2437\n",
            "Epoch [3/5], Step [288/1563], Loss: 2.1208\n",
            "Epoch [3/5], Step [320/1563], Loss: 2.4424\n",
            "Epoch [3/5], Step [352/1563], Loss: 2.4972\n",
            "Epoch [3/5], Step [384/1563], Loss: 1.9929\n",
            "Epoch [3/5], Step [416/1563], Loss: 2.3764\n",
            "Epoch [3/5], Step [448/1563], Loss: 2.1466\n",
            "Epoch [3/5], Step [480/1563], Loss: 2.4284\n",
            "Epoch [3/5], Step [512/1563], Loss: 2.2602\n",
            "Epoch [3/5], Step [544/1563], Loss: 2.3418\n",
            "Epoch [3/5], Step [576/1563], Loss: 2.1814\n",
            "Epoch [3/5], Step [608/1563], Loss: 2.0650\n",
            "Epoch [3/5], Step [640/1563], Loss: 2.0977\n",
            "Epoch [3/5], Step [672/1563], Loss: 2.2840\n",
            "Epoch [3/5], Step [704/1563], Loss: 2.3196\n",
            "Epoch [3/5], Step [736/1563], Loss: 2.1152\n",
            "Epoch [3/5], Step [768/1563], Loss: 2.1688\n",
            "Epoch [3/5], Step [800/1563], Loss: 2.3444\n",
            "Epoch [3/5], Step [832/1563], Loss: 2.4591\n",
            "Epoch [3/5], Step [864/1563], Loss: 2.0662\n",
            "Epoch [3/5], Step [896/1563], Loss: 2.1702\n",
            "Epoch [3/5], Step [928/1563], Loss: 2.2055\n",
            "Epoch [3/5], Step [960/1563], Loss: 2.2568\n",
            "Epoch [3/5], Step [992/1563], Loss: 2.1507\n",
            "Epoch [3/5], Step [1024/1563], Loss: 2.0393\n",
            "Epoch [3/5], Step [1056/1563], Loss: 2.1851\n",
            "Epoch [3/5], Step [1088/1563], Loss: 1.9520\n",
            "Epoch [3/5], Step [1120/1563], Loss: 1.9939\n",
            "Epoch [3/5], Step [1152/1563], Loss: 2.2893\n",
            "Epoch [3/5], Step [1184/1563], Loss: 2.0541\n",
            "Epoch [3/5], Step [1216/1563], Loss: 2.0786\n",
            "Epoch [3/5], Step [1248/1563], Loss: 2.1870\n",
            "Epoch [3/5], Step [1280/1563], Loss: 2.0847\n",
            "Epoch [3/5], Step [1312/1563], Loss: 2.0313\n",
            "Epoch [3/5], Step [1344/1563], Loss: 2.3600\n",
            "Epoch [3/5], Step [1376/1563], Loss: 1.9688\n",
            "Epoch [3/5], Step [1408/1563], Loss: 1.9683\n",
            "Epoch [3/5], Step [1440/1563], Loss: 2.1916\n",
            "Epoch [3/5], Step [1472/1563], Loss: 2.3471\n",
            "Epoch [3/5], Step [1504/1563], Loss: 2.1036\n",
            "Epoch [3/5], Step [1536/1563], Loss: 1.9255\n",
            "Epoch [4/5], Step [32/1563], Loss: 2.0358\n",
            "Epoch [4/5], Step [64/1563], Loss: 1.9948\n",
            "Epoch [4/5], Step [96/1563], Loss: 2.2950\n",
            "Epoch [4/5], Step [128/1563], Loss: 2.1313\n",
            "Epoch [4/5], Step [160/1563], Loss: 2.2320\n",
            "Epoch [4/5], Step [192/1563], Loss: 2.1883\n",
            "Epoch [4/5], Step [224/1563], Loss: 1.9990\n",
            "Epoch [4/5], Step [256/1563], Loss: 1.9240\n",
            "Epoch [4/5], Step [288/1563], Loss: 2.1220\n",
            "Epoch [4/5], Step [320/1563], Loss: 2.2230\n",
            "Epoch [4/5], Step [352/1563], Loss: 2.0478\n",
            "Epoch [4/5], Step [384/1563], Loss: 2.1121\n",
            "Epoch [4/5], Step [416/1563], Loss: 2.1457\n",
            "Epoch [4/5], Step [448/1563], Loss: 2.0405\n",
            "Epoch [4/5], Step [480/1563], Loss: 1.9825\n",
            "Epoch [4/5], Step [512/1563], Loss: 1.9519\n",
            "Epoch [4/5], Step [544/1563], Loss: 2.1090\n",
            "Epoch [4/5], Step [576/1563], Loss: 2.1470\n",
            "Epoch [4/5], Step [608/1563], Loss: 2.1466\n",
            "Epoch [4/5], Step [640/1563], Loss: 1.8297\n",
            "Epoch [4/5], Step [672/1563], Loss: 2.2003\n",
            "Epoch [4/5], Step [704/1563], Loss: 2.0651\n",
            "Epoch [4/5], Step [736/1563], Loss: 1.7625\n",
            "Epoch [4/5], Step [768/1563], Loss: 1.9398\n",
            "Epoch [4/5], Step [800/1563], Loss: 2.0763\n",
            "Epoch [4/5], Step [832/1563], Loss: 1.9063\n",
            "Epoch [4/5], Step [864/1563], Loss: 2.1590\n",
            "Epoch [4/5], Step [896/1563], Loss: 2.0128\n",
            "Epoch [4/5], Step [928/1563], Loss: 2.1704\n",
            "Epoch [4/5], Step [960/1563], Loss: 2.0565\n",
            "Epoch [4/5], Step [992/1563], Loss: 1.9924\n",
            "Epoch [4/5], Step [1024/1563], Loss: 2.1733\n",
            "Epoch [4/5], Step [1056/1563], Loss: 2.1121\n",
            "Epoch [4/5], Step [1088/1563], Loss: 2.2901\n",
            "Epoch [4/5], Step [1120/1563], Loss: 2.0128\n",
            "Epoch [4/5], Step [1152/1563], Loss: 2.1008\n",
            "Epoch [4/5], Step [1184/1563], Loss: 2.0352\n",
            "Epoch [4/5], Step [1216/1563], Loss: 2.0786\n",
            "Epoch [4/5], Step [1248/1563], Loss: 2.0823\n",
            "Epoch [4/5], Step [1280/1563], Loss: 2.3924\n",
            "Epoch [4/5], Step [1312/1563], Loss: 2.1223\n",
            "Epoch [4/5], Step [1344/1563], Loss: 1.9799\n",
            "Epoch [4/5], Step [1376/1563], Loss: 1.9244\n",
            "Epoch [4/5], Step [1408/1563], Loss: 1.7554\n",
            "Epoch [4/5], Step [1440/1563], Loss: 1.9993\n",
            "Epoch [4/5], Step [1472/1563], Loss: 1.7974\n",
            "Epoch [4/5], Step [1504/1563], Loss: 1.6675\n",
            "Epoch [4/5], Step [1536/1563], Loss: 1.9567\n",
            "Epoch [5/5], Step [32/1563], Loss: 1.9756\n",
            "Epoch [5/5], Step [64/1563], Loss: 1.7441\n",
            "Epoch [5/5], Step [96/1563], Loss: 2.0077\n",
            "Epoch [5/5], Step [128/1563], Loss: 2.1908\n",
            "Epoch [5/5], Step [160/1563], Loss: 2.0178\n",
            "Epoch [5/5], Step [192/1563], Loss: 2.0284\n",
            "Epoch [5/5], Step [224/1563], Loss: 2.1858\n",
            "Epoch [5/5], Step [256/1563], Loss: 1.8262\n",
            "Epoch [5/5], Step [288/1563], Loss: 1.8345\n",
            "Epoch [5/5], Step [320/1563], Loss: 1.9746\n",
            "Epoch [5/5], Step [352/1563], Loss: 1.8997\n",
            "Epoch [5/5], Step [384/1563], Loss: 1.9282\n",
            "Epoch [5/5], Step [416/1563], Loss: 1.9191\n",
            "Epoch [5/5], Step [448/1563], Loss: 1.8279\n",
            "Epoch [5/5], Step [480/1563], Loss: 2.2032\n",
            "Epoch [5/5], Step [512/1563], Loss: 1.8764\n",
            "Epoch [5/5], Step [544/1563], Loss: 1.8880\n",
            "Epoch [5/5], Step [576/1563], Loss: 1.8055\n",
            "Epoch [5/5], Step [608/1563], Loss: 2.3406\n",
            "Epoch [5/5], Step [640/1563], Loss: 2.0130\n",
            "Epoch [5/5], Step [672/1563], Loss: 2.0480\n",
            "Epoch [5/5], Step [704/1563], Loss: 1.9555\n",
            "Epoch [5/5], Step [736/1563], Loss: 1.9891\n",
            "Epoch [5/5], Step [768/1563], Loss: 2.1466\n",
            "Epoch [5/5], Step [800/1563], Loss: 2.0878\n",
            "Epoch [5/5], Step [832/1563], Loss: 1.9167\n",
            "Epoch [5/5], Step [864/1563], Loss: 2.1295\n",
            "Epoch [5/5], Step [896/1563], Loss: 1.7104\n",
            "Epoch [5/5], Step [928/1563], Loss: 2.0478\n",
            "Epoch [5/5], Step [960/1563], Loss: 2.1071\n",
            "Epoch [5/5], Step [992/1563], Loss: 2.1655\n",
            "Epoch [5/5], Step [1024/1563], Loss: 1.9657\n",
            "Epoch [5/5], Step [1056/1563], Loss: 1.9298\n",
            "Epoch [5/5], Step [1088/1563], Loss: 1.8241\n",
            "Epoch [5/5], Step [1120/1563], Loss: 1.9250\n",
            "Epoch [5/5], Step [1152/1563], Loss: 1.7084\n",
            "Epoch [5/5], Step [1184/1563], Loss: 1.8577\n",
            "Epoch [5/5], Step [1216/1563], Loss: 1.9858\n",
            "Epoch [5/5], Step [1248/1563], Loss: 1.6722\n",
            "Epoch [5/5], Step [1280/1563], Loss: 1.7083\n",
            "Epoch [5/5], Step [1312/1563], Loss: 1.8933\n",
            "Epoch [5/5], Step [1344/1563], Loss: 1.9564\n",
            "Epoch [5/5], Step [1376/1563], Loss: 2.0404\n",
            "Epoch [5/5], Step [1408/1563], Loss: 1.7609\n",
            "Epoch [5/5], Step [1440/1563], Loss: 1.8966\n",
            "Epoch [5/5], Step [1472/1563], Loss: 2.1379\n",
            "Epoch [5/5], Step [1504/1563], Loss: 2.0055\n",
            "Epoch [5/5], Step [1536/1563], Loss: 1.9602\n",
            "\n",
            "F16 precision:\n",
            "Total execution time = 107.285 sec\n",
            "Max memory used by tensors = 121830912 bytes\n",
            "Files already downloaded and verified\n",
            "Accuracy of the network on the 10000 test images: 31 %\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import argparse\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from torch import nn\n",
        "import gc\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('-n', '--nodes', default=1, type=int, metavar='N',\n",
        "                        help='number of data loading workers (default: 4)')\n",
        "    parser.add_argument('-g', '--gpus', default=1, type=int,\n",
        "                        help='number of gpus per node')\n",
        "    parser.add_argument('-nr', '--nr', default=0, type=int,\n",
        "                        help='ranking within the nodes')\n",
        "    parser.add_argument('--epochs', default=5, type=int, metavar='N',\n",
        "                        help='number of total epochs to run')\n",
        "    args = parser.parse_args([])\n",
        "    args.world_size = args.gpus * args.nodes\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '8888'\n",
        "\n",
        "    test(train(args, False, False), False) # Standard\n",
        "    test(train(args, True, False), False) # Mixed precision\n",
        "    test(train(args, False, True), True) # F16 everywhere\n",
        "\n",
        "\n",
        "def train(args, amp, f16):\n",
        "    torch.manual_seed(42)\n",
        "    model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)\n",
        "\n",
        "    if f16:\n",
        "      model = model.to(torch.float16)\n",
        "    else:\n",
        "      model = model.to(torch.float32)\n",
        "\n",
        "    model = model.cuda()\n",
        "    model.train()\n",
        "    batch_size = 32\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss().cuda()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), 1e-4)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
        "    train_dataset = torchvision.datasets.CIFAR10(root='./data',\n",
        "                                               train=True,\n",
        "                                               transform=transforms.ToTensor(),\n",
        "                                               download=True)\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=True,\n",
        "                                               num_workers=0,\n",
        "                                               pin_memory=True)\n",
        "\n",
        "    total_step = len(train_loader)\n",
        "    start_timer()\n",
        "    for epoch in range(args.epochs):\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            with torch.autocast(\"cuda\", dtype=torch.float16, enabled=(amp or f16)):\n",
        "              images = images.cuda(non_blocking=True)\n",
        "              labels = labels.cuda(non_blocking=True)\n",
        "              output = model(images)\n",
        "              loss = criterion(output, labels)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            if (i + 1) % batch_size == 0:\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, args.epochs, i + 1, total_step, loss.item()))\n",
        "\n",
        "    if f16:\n",
        "      end_timer_and_print(\"F16 precision:\")\n",
        "    elif amp:\n",
        "      end_timer_and_print(\"Mixed precision:\")\n",
        "    else:\n",
        "      end_timer_and_print(\"Standart precision:\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def test(model, f16, batch_size = 32):\n",
        "  model.eval()\n",
        "  if f16:\n",
        "    model = model.to(torch.float16)\n",
        "  else:\n",
        "    model = model.to(torch.float32)\n",
        "\n",
        "  test_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data',\n",
        "        train=False,\n",
        "        transform=transforms.ToTensor(),\n",
        "        download=True\n",
        "    )\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "      dataset=test_dataset,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False,\n",
        "      num_workers=0,\n",
        "      pin_memory=True,)\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "      for images, labels in test_loader:\n",
        "        with torch.autocast(\"cuda\", dtype=torch.float16, enabled=f16):\n",
        "          images = images.cuda(non_blocking=True)\n",
        "          labels = labels.cuda(non_blocking=True)\n",
        "          outputs = model(images)\n",
        "\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "  print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3Qjze6nJIyJ",
        "outputId": "7a816238-a130-41f1-fd19-83a9b6182ab0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Files already downloaded and verified\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "Epoch [1/5], Step [32/1563], Loss: 7.2597\n",
            "Epoch [1/5], Step [64/1563], Loss: 7.1872\n",
            "Epoch [1/5], Step [96/1563], Loss: 7.2920\n",
            "Epoch [1/5], Step [128/1563], Loss: 6.9334\n",
            "Epoch [1/5], Step [160/1563], Loss: 6.8194\n",
            "Epoch [1/5], Step [192/1563], Loss: 6.4652\n",
            "Epoch [1/5], Step [224/1563], Loss: 6.5225\n",
            "Epoch [1/5], Step [256/1563], Loss: 6.3758\n",
            "Epoch [1/5], Step [288/1563], Loss: 6.2123\n",
            "Epoch [1/5], Step [320/1563], Loss: 5.9542\n",
            "Epoch [1/5], Step [352/1563], Loss: 6.2293\n",
            "Epoch [1/5], Step [384/1563], Loss: 6.0003\n",
            "Epoch [1/5], Step [416/1563], Loss: 5.4614\n",
            "Epoch [1/5], Step [448/1563], Loss: 5.4125\n",
            "Epoch [1/5], Step [480/1563], Loss: 5.5576\n",
            "Epoch [1/5], Step [512/1563], Loss: 5.5949\n",
            "Epoch [1/5], Step [544/1563], Loss: 5.4666\n",
            "Epoch [1/5], Step [576/1563], Loss: 4.8334\n",
            "Epoch [1/5], Step [608/1563], Loss: 4.8255\n",
            "Epoch [1/5], Step [640/1563], Loss: 5.0063\n",
            "Epoch [1/5], Step [672/1563], Loss: 4.9576\n",
            "Epoch [1/5], Step [704/1563], Loss: 4.7826\n",
            "Epoch [1/5], Step [736/1563], Loss: 4.4190\n",
            "Epoch [1/5], Step [768/1563], Loss: 4.4475\n",
            "Epoch [1/5], Step [800/1563], Loss: 4.1927\n",
            "Epoch [1/5], Step [832/1563], Loss: 4.4024\n",
            "Epoch [1/5], Step [864/1563], Loss: 4.2217\n",
            "Epoch [1/5], Step [896/1563], Loss: 3.8023\n",
            "Epoch [1/5], Step [928/1563], Loss: 4.0597\n",
            "Epoch [1/5], Step [960/1563], Loss: 4.0297\n",
            "Epoch [1/5], Step [992/1563], Loss: 3.9536\n",
            "Epoch [1/5], Step [1024/1563], Loss: 3.3195\n",
            "Epoch [1/5], Step [1056/1563], Loss: 3.5764\n",
            "Epoch [1/5], Step [1088/1563], Loss: 3.7119\n",
            "Epoch [1/5], Step [1120/1563], Loss: 3.4862\n",
            "Epoch [1/5], Step [1152/1563], Loss: 3.4106\n",
            "Epoch [1/5], Step [1184/1563], Loss: 3.0699\n",
            "Epoch [1/5], Step [1216/1563], Loss: 3.3977\n",
            "Epoch [1/5], Step [1248/1563], Loss: 3.2464\n",
            "Epoch [1/5], Step [1280/1563], Loss: 3.2502\n",
            "Epoch [1/5], Step [1312/1563], Loss: 2.9898\n",
            "Epoch [1/5], Step [1344/1563], Loss: 3.2671\n",
            "Epoch [1/5], Step [1376/1563], Loss: 3.0730\n",
            "Epoch [1/5], Step [1408/1563], Loss: 3.2246\n",
            "Epoch [1/5], Step [1440/1563], Loss: 2.7136\n",
            "Epoch [1/5], Step [1472/1563], Loss: 2.8436\n",
            "Epoch [1/5], Step [1504/1563], Loss: 2.8469\n",
            "Epoch [1/5], Step [1536/1563], Loss: 2.7462\n",
            "Epoch [2/5], Step [32/1563], Loss: 2.8264\n",
            "Epoch [2/5], Step [64/1563], Loss: 2.9127\n",
            "Epoch [2/5], Step [96/1563], Loss: 2.6171\n",
            "Epoch [2/5], Step [128/1563], Loss: 2.4845\n",
            "Epoch [2/5], Step [160/1563], Loss: 2.8215\n",
            "Epoch [2/5], Step [192/1563], Loss: 2.4287\n",
            "Epoch [2/5], Step [224/1563], Loss: 2.3972\n",
            "Epoch [2/5], Step [256/1563], Loss: 2.5151\n",
            "Epoch [2/5], Step [288/1563], Loss: 2.3468\n",
            "Epoch [2/5], Step [320/1563], Loss: 2.4147\n",
            "Epoch [2/5], Step [352/1563], Loss: 2.2142\n",
            "Epoch [2/5], Step [384/1563], Loss: 2.6732\n",
            "Epoch [2/5], Step [416/1563], Loss: 2.6284\n",
            "Epoch [2/5], Step [448/1563], Loss: 2.5305\n",
            "Epoch [2/5], Step [480/1563], Loss: 2.2574\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "Epoch [2/5], Step [512/1563], Loss: 2.2591\n",
            "Epoch [2/5], Step [544/1563], Loss: 2.4193\n",
            "Epoch [2/5], Step [576/1563], Loss: 2.2975\n",
            "Epoch [2/5], Step [608/1563], Loss: 2.2898\n",
            "Epoch [2/5], Step [640/1563], Loss: 2.3919\n",
            "Epoch [2/5], Step [672/1563], Loss: 2.3963\n",
            "Epoch [2/5], Step [704/1563], Loss: 2.0925\n",
            "Epoch [2/5], Step [736/1563], Loss: 2.2713\n",
            "Epoch [2/5], Step [768/1563], Loss: 2.4526\n",
            "Epoch [2/5], Step [800/1563], Loss: 1.8376\n",
            "Epoch [2/5], Step [832/1563], Loss: 2.2082\n",
            "Epoch [2/5], Step [864/1563], Loss: 2.2892\n",
            "Epoch [2/5], Step [896/1563], Loss: 2.4425\n",
            "Epoch [2/5], Step [928/1563], Loss: 1.9133\n",
            "Epoch [2/5], Step [960/1563], Loss: 2.3338\n",
            "Epoch [2/5], Step [992/1563], Loss: 2.2234\n",
            "Epoch [2/5], Step [1024/1563], Loss: 2.2009\n",
            "Epoch [2/5], Step [1056/1563], Loss: 1.9785\n",
            "Epoch [2/5], Step [1088/1563], Loss: 2.0037\n",
            "Epoch [2/5], Step [1120/1563], Loss: 1.9689\n",
            "Epoch [2/5], Step [1152/1563], Loss: 2.0867\n",
            "Epoch [2/5], Step [1184/1563], Loss: 2.3898\n",
            "Epoch [2/5], Step [1216/1563], Loss: 2.3671\n",
            "Epoch [2/5], Step [1248/1563], Loss: 2.2520\n",
            "Epoch [2/5], Step [1280/1563], Loss: 1.8979\n",
            "Epoch [2/5], Step [1312/1563], Loss: 1.8779\n",
            "Epoch [2/5], Step [1344/1563], Loss: 2.4030\n",
            "Epoch [2/5], Step [1376/1563], Loss: 2.0699\n",
            "Epoch [2/5], Step [1408/1563], Loss: 1.9862\n",
            "Epoch [2/5], Step [1440/1563], Loss: 2.2572\n",
            "Epoch [2/5], Step [1472/1563], Loss: 2.0386\n",
            "Epoch [2/5], Step [1504/1563], Loss: 2.0365\n",
            "Epoch [2/5], Step [1536/1563], Loss: 2.1859\n",
            "Epoch [3/5], Step [32/1563], Loss: 2.0467\n",
            "Epoch [3/5], Step [64/1563], Loss: 2.0098\n",
            "Epoch [3/5], Step [96/1563], Loss: 2.1109\n",
            "Epoch [3/5], Step [128/1563], Loss: 2.1212\n",
            "Epoch [3/5], Step [160/1563], Loss: 2.0327\n",
            "Epoch [3/5], Step [192/1563], Loss: 2.0691\n",
            "Epoch [3/5], Step [224/1563], Loss: 1.8480\n",
            "Epoch [3/5], Step [256/1563], Loss: 1.9329\n",
            "Epoch [3/5], Step [288/1563], Loss: 1.7715\n",
            "Epoch [3/5], Step [320/1563], Loss: 2.2068\n",
            "Epoch [3/5], Step [352/1563], Loss: 2.1264\n",
            "Epoch [3/5], Step [384/1563], Loss: 1.6698\n",
            "Epoch [3/5], Step [416/1563], Loss: 2.2072\n",
            "Epoch [3/5], Step [448/1563], Loss: 1.7668\n",
            "Epoch [3/5], Step [480/1563], Loss: 2.0542\n",
            "Epoch [3/5], Step [512/1563], Loss: 1.9475\n",
            "Epoch [3/5], Step [544/1563], Loss: 1.7657\n",
            "Epoch [3/5], Step [576/1563], Loss: 1.5559\n",
            "Epoch [3/5], Step [608/1563], Loss: 1.9405\n",
            "Epoch [3/5], Step [640/1563], Loss: 1.8709\n",
            "Epoch [3/5], Step [672/1563], Loss: 1.8634\n",
            "Epoch [3/5], Step [704/1563], Loss: 2.3246\n",
            "Epoch [3/5], Step [736/1563], Loss: 1.8530\n",
            "Epoch [3/5], Step [768/1563], Loss: 1.8855\n",
            "Epoch [3/5], Step [800/1563], Loss: 1.9508\n",
            "Epoch [3/5], Step [832/1563], Loss: 2.2113\n",
            "Epoch [3/5], Step [864/1563], Loss: 1.8008\n",
            "Epoch [3/5], Step [896/1563], Loss: 1.7984\n",
            "Epoch [3/5], Step [928/1563], Loss: 1.7160\n",
            "Epoch [3/5], Step [960/1563], Loss: 1.8140\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "Epoch [3/5], Step [992/1563], Loss: 2.0073\n",
            "Epoch [3/5], Step [1024/1563], Loss: 1.7565\n",
            "Epoch [3/5], Step [1056/1563], Loss: 1.9061\n",
            "Epoch [3/5], Step [1088/1563], Loss: 1.8473\n",
            "Epoch [3/5], Step [1120/1563], Loss: 1.7156\n",
            "Epoch [3/5], Step [1152/1563], Loss: 2.0617\n",
            "Epoch [3/5], Step [1184/1563], Loss: 2.0346\n",
            "Epoch [3/5], Step [1216/1563], Loss: 2.0789\n",
            "Epoch [3/5], Step [1248/1563], Loss: 1.8371\n",
            "Epoch [3/5], Step [1280/1563], Loss: 1.9865\n",
            "Epoch [3/5], Step [1312/1563], Loss: 1.6751\n",
            "Epoch [3/5], Step [1344/1563], Loss: 1.8426\n",
            "Epoch [3/5], Step [1376/1563], Loss: 1.7015\n",
            "Epoch [3/5], Step [1408/1563], Loss: 1.6177\n",
            "Epoch [3/5], Step [1440/1563], Loss: 1.9344\n",
            "Epoch [3/5], Step [1472/1563], Loss: 2.0629\n",
            "Epoch [3/5], Step [1504/1563], Loss: 1.8506\n",
            "Epoch [3/5], Step [1536/1563], Loss: 1.7507\n",
            "Epoch [4/5], Step [32/1563], Loss: 1.7898\n",
            "Epoch [4/5], Step [64/1563], Loss: 1.8186\n",
            "Epoch [4/5], Step [96/1563], Loss: 2.0881\n",
            "Epoch [4/5], Step [128/1563], Loss: 1.8750\n",
            "Epoch [4/5], Step [160/1563], Loss: 1.9423\n",
            "Epoch [4/5], Step [192/1563], Loss: 1.8296\n",
            "Epoch [4/5], Step [224/1563], Loss: 1.5172\n",
            "Epoch [4/5], Step [256/1563], Loss: 1.6399\n",
            "Epoch [4/5], Step [288/1563], Loss: 1.8383\n",
            "Epoch [4/5], Step [320/1563], Loss: 1.9765\n",
            "Epoch [4/5], Step [352/1563], Loss: 1.8346\n",
            "Epoch [4/5], Step [384/1563], Loss: 1.7256\n",
            "Epoch [4/5], Step [416/1563], Loss: 1.9431\n",
            "Epoch [4/5], Step [448/1563], Loss: 1.9020\n",
            "Epoch [4/5], Step [480/1563], Loss: 1.8534\n",
            "Epoch [4/5], Step [512/1563], Loss: 1.6584\n",
            "Epoch [4/5], Step [544/1563], Loss: 1.8919\n",
            "Epoch [4/5], Step [576/1563], Loss: 1.7516\n",
            "Epoch [4/5], Step [608/1563], Loss: 1.8033\n",
            "Epoch [4/5], Step [640/1563], Loss: 1.8057\n",
            "Epoch [4/5], Step [672/1563], Loss: 2.0323\n",
            "Epoch [4/5], Step [704/1563], Loss: 1.8452\n",
            "Epoch [4/5], Step [736/1563], Loss: 1.4022\n",
            "Epoch [4/5], Step [768/1563], Loss: 1.7731\n",
            "Epoch [4/5], Step [800/1563], Loss: 1.8196\n",
            "Epoch [4/5], Step [832/1563], Loss: 1.6633\n",
            "Epoch [4/5], Step [864/1563], Loss: 1.6364\n",
            "Epoch [4/5], Step [896/1563], Loss: 1.6853\n",
            "Epoch [4/5], Step [928/1563], Loss: 2.0838\n",
            "Epoch [4/5], Step [960/1563], Loss: 1.6764\n",
            "Epoch [4/5], Step [992/1563], Loss: 1.8445\n",
            "Epoch [4/5], Step [1024/1563], Loss: 1.7127\n",
            "Epoch [4/5], Step [1056/1563], Loss: 1.8911\n",
            "Epoch [4/5], Step [1088/1563], Loss: 1.9441\n",
            "Epoch [4/5], Step [1120/1563], Loss: 1.8182\n",
            "Epoch [4/5], Step [1152/1563], Loss: 1.8516\n",
            "Epoch [4/5], Step [1184/1563], Loss: 1.7522\n",
            "Epoch [4/5], Step [1216/1563], Loss: 1.7025\n",
            "Epoch [4/5], Step [1248/1563], Loss: 1.7138\n",
            "Epoch [4/5], Step [1280/1563], Loss: 2.2372\n",
            "Epoch [4/5], Step [1312/1563], Loss: 1.8663\n",
            "Epoch [4/5], Step [1344/1563], Loss: 1.8077\n",
            "Epoch [4/5], Step [1376/1563], Loss: 1.6095\n",
            "Epoch [4/5], Step [1408/1563], Loss: 1.4019\n",
            "Epoch [4/5], Step [1440/1563], Loss: 1.6989\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "Epoch [4/5], Step [1472/1563], Loss: 1.6318\n",
            "Epoch [4/5], Step [1504/1563], Loss: 1.3120\n",
            "Epoch [4/5], Step [1536/1563], Loss: 1.7071\n",
            "Epoch [5/5], Step [32/1563], Loss: 1.7448\n",
            "Epoch [5/5], Step [64/1563], Loss: 1.4820\n",
            "Epoch [5/5], Step [96/1563], Loss: 1.6598\n",
            "Epoch [5/5], Step [128/1563], Loss: 1.9288\n",
            "Epoch [5/5], Step [160/1563], Loss: 1.5254\n",
            "Epoch [5/5], Step [192/1563], Loss: 1.6896\n",
            "Epoch [5/5], Step [224/1563], Loss: 1.9474\n",
            "Epoch [5/5], Step [256/1563], Loss: 1.3426\n",
            "Epoch [5/5], Step [288/1563], Loss: 1.6701\n",
            "Epoch [5/5], Step [320/1563], Loss: 1.6386\n",
            "Epoch [5/5], Step [352/1563], Loss: 1.5323\n",
            "Epoch [5/5], Step [384/1563], Loss: 1.7352\n",
            "Epoch [5/5], Step [416/1563], Loss: 1.6203\n",
            "Epoch [5/5], Step [448/1563], Loss: 1.6244\n",
            "Epoch [5/5], Step [480/1563], Loss: 1.9002\n",
            "Epoch [5/5], Step [512/1563], Loss: 1.5903\n",
            "Epoch [5/5], Step [544/1563], Loss: 1.4728\n",
            "Epoch [5/5], Step [576/1563], Loss: 1.6071\n",
            "Epoch [5/5], Step [608/1563], Loss: 2.0425\n",
            "Epoch [5/5], Step [640/1563], Loss: 1.7151\n",
            "Epoch [5/5], Step [672/1563], Loss: 1.8125\n",
            "Epoch [5/5], Step [704/1563], Loss: 1.7600\n",
            "Epoch [5/5], Step [736/1563], Loss: 1.8868\n",
            "Epoch [5/5], Step [768/1563], Loss: 1.6439\n",
            "Epoch [5/5], Step [800/1563], Loss: 1.5373\n",
            "Epoch [5/5], Step [832/1563], Loss: 1.7003\n",
            "Epoch [5/5], Step [864/1563], Loss: 1.8793\n",
            "Epoch [5/5], Step [896/1563], Loss: 1.4244\n",
            "Epoch [5/5], Step [928/1563], Loss: 1.7187\n",
            "Epoch [5/5], Step [960/1563], Loss: 1.8893\n",
            "Epoch [5/5], Step [992/1563], Loss: 1.7493\n",
            "Epoch [5/5], Step [1024/1563], Loss: 1.7664\n",
            "Epoch [5/5], Step [1056/1563], Loss: 1.7172\n",
            "Epoch [5/5], Step [1088/1563], Loss: 1.5450\n",
            "Epoch [5/5], Step [1120/1563], Loss: 1.6302\n",
            "Epoch [5/5], Step [1152/1563], Loss: 1.4469\n",
            "Epoch [5/5], Step [1184/1563], Loss: 1.4846\n",
            "Epoch [5/5], Step [1216/1563], Loss: 1.7590\n",
            "Epoch [5/5], Step [1248/1563], Loss: 1.5757\n",
            "Epoch [5/5], Step [1280/1563], Loss: 1.4656\n",
            "Epoch [5/5], Step [1312/1563], Loss: 1.5950\n",
            "Epoch [5/5], Step [1344/1563], Loss: 1.6398\n",
            "Epoch [5/5], Step [1376/1563], Loss: 2.0883\n",
            "Epoch [5/5], Step [1408/1563], Loss: 1.3876\n",
            "Epoch [5/5], Step [1440/1563], Loss: 1.5983\n",
            "Epoch [5/5], Step [1472/1563], Loss: 2.0771\n",
            "Epoch [5/5], Step [1504/1563], Loss: 1.7517\n",
            "Epoch [5/5], Step [1536/1563], Loss: 1.6360\n",
            "\n",
            "Mixed precision:\n",
            "Total execution time = 130.432 sec\n",
            "Max memory used by tensors = 161272832 bytes\n",
            "Files already downloaded and verified\n",
            "Accuracy of the network on the 10000 test images: 40 %\n",
            "Selected optimization level O3:  Pure FP16 training.\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O3\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : False\n",
            "master_weights         : False\n",
            "loss_scale             : 1.0\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O3\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : False\n",
            "master_weights         : False\n",
            "loss_scale             : 1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Epoch [1/5], Step [32/1563], Loss: 7.2834\n",
            "Epoch [1/5], Step [64/1563], Loss: 7.1748\n",
            "Epoch [1/5], Step [96/1563], Loss: 7.3359\n",
            "Epoch [1/5], Step [128/1563], Loss: 7.0357\n",
            "Epoch [1/5], Step [160/1563], Loss: 6.8382\n",
            "Epoch [1/5], Step [192/1563], Loss: 6.5858\n",
            "Epoch [1/5], Step [224/1563], Loss: 6.8189\n",
            "Epoch [1/5], Step [256/1563], Loss: 6.6004\n",
            "Epoch [1/5], Step [288/1563], Loss: 6.4512\n",
            "Epoch [1/5], Step [320/1563], Loss: 6.3442\n",
            "Epoch [1/5], Step [352/1563], Loss: 6.4114\n",
            "Epoch [1/5], Step [384/1563], Loss: 6.2304\n",
            "Epoch [1/5], Step [416/1563], Loss: 5.9504\n",
            "Epoch [1/5], Step [448/1563], Loss: 5.8842\n",
            "Epoch [1/5], Step [480/1563], Loss: 5.9356\n",
            "Epoch [1/5], Step [512/1563], Loss: 5.9028\n",
            "Epoch [1/5], Step [544/1563], Loss: 5.8081\n",
            "Epoch [1/5], Step [576/1563], Loss: 5.4526\n",
            "Epoch [1/5], Step [608/1563], Loss: 5.5709\n",
            "Epoch [1/5], Step [640/1563], Loss: 5.5099\n",
            "Epoch [1/5], Step [672/1563], Loss: 5.4788\n",
            "Epoch [1/5], Step [704/1563], Loss: 5.2500\n",
            "Epoch [1/5], Step [736/1563], Loss: 4.9429\n",
            "Epoch [1/5], Step [768/1563], Loss: 5.0273\n",
            "Epoch [1/5], Step [800/1563], Loss: 4.8231\n",
            "Epoch [1/5], Step [832/1563], Loss: 4.9693\n",
            "Epoch [1/5], Step [864/1563], Loss: 4.8548\n",
            "Epoch [1/5], Step [896/1563], Loss: 4.5956\n",
            "Epoch [1/5], Step [928/1563], Loss: 4.7080\n",
            "Epoch [1/5], Step [960/1563], Loss: 4.8029\n",
            "Epoch [1/5], Step [992/1563], Loss: 4.6244\n",
            "Epoch [1/5], Step [1024/1563], Loss: 4.2899\n",
            "Epoch [1/5], Step [1056/1563], Loss: 4.3221\n",
            "Epoch [1/5], Step [1088/1563], Loss: 4.2149\n",
            "Epoch [1/5], Step [1120/1563], Loss: 4.2170\n",
            "Epoch [1/5], Step [1152/1563], Loss: 4.1016\n",
            "Epoch [1/5], Step [1184/1563], Loss: 3.8780\n",
            "Epoch [1/5], Step [1216/1563], Loss: 3.9660\n",
            "Epoch [1/5], Step [1248/1563], Loss: 4.0576\n",
            "Epoch [1/5], Step [1280/1563], Loss: 3.8685\n",
            "Epoch [1/5], Step [1312/1563], Loss: 3.6849\n",
            "Epoch [1/5], Step [1344/1563], Loss: 3.8578\n",
            "Epoch [1/5], Step [1376/1563], Loss: 3.8035\n",
            "Epoch [1/5], Step [1408/1563], Loss: 3.8184\n",
            "Epoch [1/5], Step [1440/1563], Loss: 3.5418\n",
            "Epoch [1/5], Step [1472/1563], Loss: 3.5888\n",
            "Epoch [1/5], Step [1504/1563], Loss: 3.3988\n",
            "Epoch [1/5], Step [1536/1563], Loss: 3.4507\n",
            "Epoch [2/5], Step [32/1563], Loss: 3.4615\n",
            "Epoch [2/5], Step [64/1563], Loss: 3.5183\n",
            "Epoch [2/5], Step [96/1563], Loss: 3.1917\n",
            "Epoch [2/5], Step [128/1563], Loss: 3.2000\n",
            "Epoch [2/5], Step [160/1563], Loss: 3.0964\n",
            "Epoch [2/5], Step [192/1563], Loss: 3.1038\n",
            "Epoch [2/5], Step [224/1563], Loss: 3.1984\n",
            "Epoch [2/5], Step [256/1563], Loss: 3.0695\n",
            "Epoch [2/5], Step [288/1563], Loss: 3.0026\n",
            "Epoch [2/5], Step [320/1563], Loss: 2.9978\n",
            "Epoch [2/5], Step [352/1563], Loss: 3.1059\n",
            "Epoch [2/5], Step [384/1563], Loss: 3.0749\n",
            "Epoch [2/5], Step [416/1563], Loss: 3.1053\n",
            "Epoch [2/5], Step [448/1563], Loss: 2.9678\n",
            "Epoch [2/5], Step [480/1563], Loss: 2.8721\n",
            "Epoch [2/5], Step [512/1563], Loss: 2.8659\n",
            "Epoch [2/5], Step [544/1563], Loss: 2.9447\n",
            "Epoch [2/5], Step [576/1563], Loss: 2.8450\n",
            "Epoch [2/5], Step [608/1563], Loss: 2.8514\n",
            "Epoch [2/5], Step [640/1563], Loss: 2.7826\n",
            "Epoch [2/5], Step [672/1563], Loss: 2.9883\n",
            "Epoch [2/5], Step [704/1563], Loss: 2.5483\n",
            "Epoch [2/5], Step [736/1563], Loss: 2.6802\n",
            "Epoch [2/5], Step [768/1563], Loss: 2.6570\n",
            "Epoch [2/5], Step [800/1563], Loss: 2.4253\n",
            "Epoch [2/5], Step [832/1563], Loss: 2.5868\n",
            "Epoch [2/5], Step [864/1563], Loss: 2.6012\n",
            "Epoch [2/5], Step [896/1563], Loss: 2.8399\n",
            "Epoch [2/5], Step [928/1563], Loss: 2.4252\n",
            "Epoch [2/5], Step [960/1563], Loss: 2.6507\n",
            "Epoch [2/5], Step [992/1563], Loss: 2.4917\n",
            "Epoch [2/5], Step [1024/1563], Loss: 2.6489\n",
            "Epoch [2/5], Step [1056/1563], Loss: 2.3758\n",
            "Epoch [2/5], Step [1088/1563], Loss: 2.3738\n",
            "Epoch [2/5], Step [1120/1563], Loss: 2.4563\n",
            "Epoch [2/5], Step [1152/1563], Loss: 2.4275\n",
            "Epoch [2/5], Step [1184/1563], Loss: 2.6009\n",
            "Epoch [2/5], Step [1216/1563], Loss: 2.5334\n",
            "Epoch [2/5], Step [1248/1563], Loss: 2.5474\n",
            "Epoch [2/5], Step [1280/1563], Loss: 2.2606\n",
            "Epoch [2/5], Step [1312/1563], Loss: 2.3483\n",
            "Epoch [2/5], Step [1344/1563], Loss: 2.5479\n",
            "Epoch [2/5], Step [1376/1563], Loss: 2.4598\n",
            "Epoch [2/5], Step [1408/1563], Loss: 2.3360\n",
            "Epoch [2/5], Step [1440/1563], Loss: 2.3662\n",
            "Epoch [2/5], Step [1472/1563], Loss: 2.3830\n",
            "Epoch [2/5], Step [1504/1563], Loss: 2.4039\n",
            "Epoch [2/5], Step [1536/1563], Loss: 2.3696\n",
            "Epoch [3/5], Step [32/1563], Loss: 2.3547\n",
            "Epoch [3/5], Step [64/1563], Loss: 2.2291\n",
            "Epoch [3/5], Step [96/1563], Loss: 2.2605\n",
            "Epoch [3/5], Step [128/1563], Loss: 2.4502\n",
            "Epoch [3/5], Step [160/1563], Loss: 2.1455\n",
            "Epoch [3/5], Step [192/1563], Loss: 2.3912\n",
            "Epoch [3/5], Step [224/1563], Loss: 2.1962\n",
            "Epoch [3/5], Step [256/1563], Loss: 2.2158\n",
            "Epoch [3/5], Step [288/1563], Loss: 2.1345\n",
            "Epoch [3/5], Step [320/1563], Loss: 2.5731\n",
            "Epoch [3/5], Step [352/1563], Loss: 2.4011\n",
            "Epoch [3/5], Step [384/1563], Loss: 1.9227\n",
            "Epoch [3/5], Step [416/1563], Loss: 2.4414\n",
            "Epoch [3/5], Step [448/1563], Loss: 2.0429\n",
            "Epoch [3/5], Step [480/1563], Loss: 2.3742\n",
            "Epoch [3/5], Step [512/1563], Loss: 2.2136\n",
            "Epoch [3/5], Step [544/1563], Loss: 2.1934\n",
            "Epoch [3/5], Step [576/1563], Loss: 2.0412\n",
            "Epoch [3/5], Step [608/1563], Loss: 2.0324\n",
            "Epoch [3/5], Step [640/1563], Loss: 2.0520\n",
            "Epoch [3/5], Step [672/1563], Loss: 2.3660\n",
            "Epoch [3/5], Step [704/1563], Loss: 2.4719\n",
            "Epoch [3/5], Step [736/1563], Loss: 2.2574\n",
            "Epoch [3/5], Step [768/1563], Loss: 2.1721\n",
            "Epoch [3/5], Step [800/1563], Loss: 2.2574\n",
            "Epoch [3/5], Step [832/1563], Loss: 2.2047\n",
            "Epoch [3/5], Step [864/1563], Loss: 2.0114\n",
            "Epoch [3/5], Step [896/1563], Loss: 2.1862\n",
            "Epoch [3/5], Step [928/1563], Loss: 2.2159\n",
            "Epoch [3/5], Step [960/1563], Loss: 2.0856\n",
            "Epoch [3/5], Step [992/1563], Loss: 2.1462\n",
            "Epoch [3/5], Step [1024/1563], Loss: 2.1044\n",
            "Epoch [3/5], Step [1056/1563], Loss: 2.1683\n",
            "Epoch [3/5], Step [1088/1563], Loss: 2.0281\n",
            "Epoch [3/5], Step [1120/1563], Loss: 2.0359\n",
            "Epoch [3/5], Step [1152/1563], Loss: 2.3035\n",
            "Epoch [3/5], Step [1184/1563], Loss: 2.1272\n",
            "Epoch [3/5], Step [1216/1563], Loss: 2.2144\n",
            "Epoch [3/5], Step [1248/1563], Loss: 2.1177\n",
            "Epoch [3/5], Step [1280/1563], Loss: 2.2038\n",
            "Epoch [3/5], Step [1312/1563], Loss: 2.0983\n",
            "Epoch [3/5], Step [1344/1563], Loss: 2.3958\n",
            "Epoch [3/5], Step [1376/1563], Loss: 2.1061\n",
            "Epoch [3/5], Step [1408/1563], Loss: 2.0369\n",
            "Epoch [3/5], Step [1440/1563], Loss: 2.3629\n",
            "Epoch [3/5], Step [1472/1563], Loss: 2.1851\n",
            "Epoch [3/5], Step [1504/1563], Loss: 2.1887\n",
            "Epoch [3/5], Step [1536/1563], Loss: 1.9162\n",
            "Epoch [4/5], Step [32/1563], Loss: 1.9694\n",
            "Epoch [4/5], Step [64/1563], Loss: 2.0702\n",
            "Epoch [4/5], Step [96/1563], Loss: 2.2823\n",
            "Epoch [4/5], Step [128/1563], Loss: 2.1545\n",
            "Epoch [4/5], Step [160/1563], Loss: 2.2797\n",
            "Epoch [4/5], Step [192/1563], Loss: 2.1581\n",
            "Epoch [4/5], Step [224/1563], Loss: 1.8380\n",
            "Epoch [4/5], Step [256/1563], Loss: 1.9769\n",
            "Epoch [4/5], Step [288/1563], Loss: 2.0669\n",
            "Epoch [4/5], Step [320/1563], Loss: 2.1033\n",
            "Epoch [4/5], Step [352/1563], Loss: 2.2448\n",
            "Epoch [4/5], Step [384/1563], Loss: 2.1866\n",
            "Epoch [4/5], Step [416/1563], Loss: 2.1963\n",
            "Epoch [4/5], Step [448/1563], Loss: 2.0870\n",
            "Epoch [4/5], Step [480/1563], Loss: 1.9773\n",
            "Epoch [4/5], Step [512/1563], Loss: 2.0044\n",
            "Epoch [4/5], Step [544/1563], Loss: 2.0532\n",
            "Epoch [4/5], Step [576/1563], Loss: 2.0854\n",
            "Epoch [4/5], Step [608/1563], Loss: 2.2713\n",
            "Epoch [4/5], Step [640/1563], Loss: 1.9466\n",
            "Epoch [4/5], Step [672/1563], Loss: 2.1891\n",
            "Epoch [4/5], Step [704/1563], Loss: 1.9627\n",
            "Epoch [4/5], Step [736/1563], Loss: 1.6931\n",
            "Epoch [4/5], Step [768/1563], Loss: 1.9122\n",
            "Epoch [4/5], Step [800/1563], Loss: 1.9809\n",
            "Epoch [4/5], Step [832/1563], Loss: 1.8837\n",
            "Epoch [4/5], Step [864/1563], Loss: 1.9749\n",
            "Epoch [4/5], Step [896/1563], Loss: 1.8948\n",
            "Epoch [4/5], Step [928/1563], Loss: 2.4623\n",
            "Epoch [4/5], Step [960/1563], Loss: 2.0300\n",
            "Epoch [4/5], Step [992/1563], Loss: 1.8267\n",
            "Epoch [4/5], Step [1024/1563], Loss: 2.2167\n",
            "Epoch [4/5], Step [1056/1563], Loss: 2.1443\n",
            "Epoch [4/5], Step [1088/1563], Loss: 2.2386\n",
            "Epoch [4/5], Step [1120/1563], Loss: 2.0854\n",
            "Epoch [4/5], Step [1152/1563], Loss: 2.0027\n",
            "Epoch [4/5], Step [1184/1563], Loss: 1.9601\n",
            "Epoch [4/5], Step [1216/1563], Loss: 2.1238\n",
            "Epoch [4/5], Step [1248/1563], Loss: 2.0515\n",
            "Epoch [4/5], Step [1280/1563], Loss: 2.2421\n",
            "Epoch [4/5], Step [1312/1563], Loss: 1.9535\n",
            "Epoch [4/5], Step [1344/1563], Loss: 1.8540\n",
            "Epoch [4/5], Step [1376/1563], Loss: 1.8222\n",
            "Epoch [4/5], Step [1408/1563], Loss: 1.7568\n",
            "Epoch [4/5], Step [1440/1563], Loss: 1.8821\n",
            "Epoch [4/5], Step [1472/1563], Loss: 1.8226\n",
            "Epoch [4/5], Step [1504/1563], Loss: 1.6687\n",
            "Epoch [4/5], Step [1536/1563], Loss: 1.9233\n",
            "Epoch [5/5], Step [32/1563], Loss: 1.9755\n",
            "Epoch [5/5], Step [64/1563], Loss: 1.9268\n",
            "Epoch [5/5], Step [96/1563], Loss: 1.9541\n",
            "Epoch [5/5], Step [128/1563], Loss: 2.1775\n",
            "Epoch [5/5], Step [160/1563], Loss: 1.7303\n",
            "Epoch [5/5], Step [192/1563], Loss: 1.8891\n",
            "Epoch [5/5], Step [224/1563], Loss: 2.2568\n",
            "Epoch [5/5], Step [256/1563], Loss: 1.8119\n",
            "Epoch [5/5], Step [288/1563], Loss: 1.8120\n",
            "Epoch [5/5], Step [320/1563], Loss: 1.9207\n",
            "Epoch [5/5], Step [352/1563], Loss: 1.8115\n",
            "Epoch [5/5], Step [384/1563], Loss: 1.9943\n",
            "Epoch [5/5], Step [416/1563], Loss: 1.8160\n",
            "Epoch [5/5], Step [448/1563], Loss: 1.8815\n",
            "Epoch [5/5], Step [480/1563], Loss: 2.0980\n",
            "Epoch [5/5], Step [512/1563], Loss: 1.8470\n",
            "Epoch [5/5], Step [544/1563], Loss: 1.8731\n",
            "Epoch [5/5], Step [576/1563], Loss: 1.9690\n",
            "Epoch [5/5], Step [608/1563], Loss: 2.3608\n",
            "Epoch [5/5], Step [640/1563], Loss: 2.0350\n",
            "Epoch [5/5], Step [672/1563], Loss: 2.0449\n",
            "Epoch [5/5], Step [704/1563], Loss: 1.9551\n",
            "Epoch [5/5], Step [736/1563], Loss: 1.9464\n",
            "Epoch [5/5], Step [768/1563], Loss: 1.8848\n",
            "Epoch [5/5], Step [800/1563], Loss: 2.0404\n",
            "Epoch [5/5], Step [832/1563], Loss: 1.9261\n",
            "Epoch [5/5], Step [864/1563], Loss: 2.2087\n",
            "Epoch [5/5], Step [896/1563], Loss: 1.6165\n",
            "Epoch [5/5], Step [928/1563], Loss: 1.8533\n",
            "Epoch [5/5], Step [960/1563], Loss: 1.8851\n",
            "Epoch [5/5], Step [992/1563], Loss: 2.2283\n",
            "Epoch [5/5], Step [1024/1563], Loss: 2.1332\n",
            "Epoch [5/5], Step [1056/1563], Loss: 1.8785\n",
            "Epoch [5/5], Step [1088/1563], Loss: 1.8332\n",
            "Epoch [5/5], Step [1120/1563], Loss: 1.9443\n",
            "Epoch [5/5], Step [1152/1563], Loss: 1.8233\n",
            "Epoch [5/5], Step [1184/1563], Loss: 1.8336\n",
            "Epoch [5/5], Step [1216/1563], Loss: 1.9313\n",
            "Epoch [5/5], Step [1248/1563], Loss: 1.9563\n",
            "Epoch [5/5], Step [1280/1563], Loss: 1.6176\n",
            "Epoch [5/5], Step [1312/1563], Loss: 1.7850\n",
            "Epoch [5/5], Step [1344/1563], Loss: 1.8200\n",
            "Epoch [5/5], Step [1376/1563], Loss: 2.0719\n",
            "Epoch [5/5], Step [1408/1563], Loss: 1.8544\n",
            "Epoch [5/5], Step [1440/1563], Loss: 1.9656\n",
            "Epoch [5/5], Step [1472/1563], Loss: 2.1478\n",
            "Epoch [5/5], Step [1504/1563], Loss: 2.0004\n",
            "Epoch [5/5], Step [1536/1563], Loss: 2.0152\n",
            "\n",
            "F16 precision:\n",
            "Total execution time = 121.288 sec\n",
            "Max memory used by tensors = 78595584 bytes\n",
            "Files already downloaded and verified\n",
            "Accuracy of the network on the 10000 test images: 31 %\n"
          ]
        }
      ],
      "source": [
        "from apex import amp as amp_lib\n",
        "import os\n",
        "from datetime import datetime\n",
        "import argparse\n",
        "import torch.multiprocessing as mp\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.distributed as dist\n",
        "import apex\n",
        "import gc\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('-n', '--nodes', default=1, type=int, metavar='N',\n",
        "                        help='number of data loading workers (default: 4)')\n",
        "    parser.add_argument('-g', '--gpus', default=1, type=int,\n",
        "                        help='number of gpus per node')\n",
        "    parser.add_argument('-nr', '--nr', default=0, type=int,\n",
        "                        help='ranking within the nodes')\n",
        "    parser.add_argument('--epochs', default=5, type=int, metavar='N',\n",
        "                        help='number of total epochs to run')\n",
        "    args = parser.parse_args([])\n",
        "    args.world_size = args.gpus * args.nodes\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '8888'\n",
        "\n",
        "    test(train(args, True, False), False) # Mixed precision\n",
        "    test(train(args, False, True), True) # F16 everywhere\n",
        "\n",
        "\n",
        "def train(args, amp, f16):\n",
        "    torch.manual_seed(42)\n",
        "    model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)\n",
        "    model = model.to(torch.float32)\n",
        "    model = model.cuda()\n",
        "    batch_size = 32\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss().cuda()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), 1e-4)\n",
        "    if amp:\n",
        "      opt_level = \"O2\"\n",
        "    elif f16:\n",
        "      opt_level = \"O3\"\n",
        "    model, optimizer = amp_lib.initialize(model, optimizer, opt_level=opt_level)\n",
        "    train_dataset = torchvision.datasets.CIFAR10(root='./data',\n",
        "                                               train=True,\n",
        "                                               transform=transforms.ToTensor(),\n",
        "                                               download=True)\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=True,\n",
        "                                               num_workers=0,\n",
        "                                               pin_memory=True)\n",
        "\n",
        "    total_step = len(train_loader)\n",
        "    start_timer()\n",
        "    for epoch in range(args.epochs):\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images = images.cuda(non_blocking=True)\n",
        "            labels = labels.cuda(non_blocking=True)\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            with amp_lib.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                scaled_loss.backward()\n",
        "            optimizer.step()\n",
        "            if (i + 1) % batch_size == 0:\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(\n",
        "                    epoch + 1,\n",
        "                    args.epochs,\n",
        "                    i + 1,\n",
        "                    total_step,\n",
        "                    loss.item()))\n",
        "\n",
        "    if f16:\n",
        "      end_timer_and_print(\"F16 precision:\")\n",
        "    elif amp:\n",
        "      end_timer_and_print(\"Mixed precision:\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def test(model, f16, batch_size = 32):\n",
        "  test_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data',\n",
        "        train=False,\n",
        "        transform=transforms.ToTensor(),\n",
        "        download=True\n",
        "    )\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "      dataset=test_dataset,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False,\n",
        "      num_workers=0,\n",
        "      pin_memory=True,)\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "      images = images.cuda(non_blocking=True)\n",
        "      labels = labels.cuda(non_blocking=True)\n",
        "      outputs = model(images)\n",
        "\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "  print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrziQH4LKuhg",
        "outputId": "567a11f9-32b2-4ca8-8cad-0741d54e6922"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Files already downloaded and verified\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Epoch [1/15], Step [100/500], Loss: 6.9227\n",
            "Epoch [1/15], Step [200/500], Loss: 6.6222\n",
            "Epoch [1/15], Step [300/500], Loss: 6.2470\n",
            "Epoch [1/15], Step [400/500], Loss: 5.9257\n",
            "Epoch [1/15], Step [500/500], Loss: 5.5271\n",
            "Epoch [2/15], Step [100/500], Loss: 5.1665\n",
            "Epoch [2/15], Step [200/500], Loss: 4.6391\n",
            "Epoch [2/15], Step [300/500], Loss: 4.2576\n",
            "Epoch [2/15], Step [400/500], Loss: 4.2456\n",
            "Epoch [2/15], Step [500/500], Loss: 3.8479\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "Epoch [3/15], Step [100/500], Loss: 3.5950\n",
            "Epoch [3/15], Step [200/500], Loss: 3.3599\n",
            "Epoch [3/15], Step [300/500], Loss: 3.1888\n",
            "Epoch [3/15], Step [400/500], Loss: 2.9637\n",
            "Epoch [3/15], Step [500/500], Loss: 2.8085\n",
            "Epoch [4/15], Step [100/500], Loss: 2.8607\n",
            "Epoch [4/15], Step [200/500], Loss: 2.4544\n",
            "Epoch [4/15], Step [300/500], Loss: 2.5778\n",
            "Epoch [4/15], Step [400/500], Loss: 2.4234\n",
            "Epoch [4/15], Step [500/500], Loss: 2.3195\n",
            "Epoch [5/15], Step [100/500], Loss: 2.1669\n",
            "Epoch [5/15], Step [200/500], Loss: 2.3844\n",
            "Epoch [5/15], Step [300/500], Loss: 2.1325\n",
            "Epoch [5/15], Step [400/500], Loss: 2.1359\n",
            "Epoch [5/15], Step [500/500], Loss: 2.1966\n",
            "Epoch [6/15], Step [100/500], Loss: 2.1254\n",
            "Epoch [6/15], Step [200/500], Loss: 2.0069\n",
            "Epoch [6/15], Step [300/500], Loss: 2.1260\n",
            "Epoch [6/15], Step [400/500], Loss: 2.0018\n",
            "Epoch [6/15], Step [500/500], Loss: 2.0769\n",
            "Epoch [7/15], Step [100/500], Loss: 1.9985\n",
            "Epoch [7/15], Step [200/500], Loss: 1.7986\n",
            "Epoch [7/15], Step [300/500], Loss: 1.7954\n",
            "Epoch [7/15], Step [400/500], Loss: 2.1173\n",
            "Epoch [7/15], Step [500/500], Loss: 1.9016\n",
            "Epoch [8/15], Step [100/500], Loss: 1.9925\n",
            "Epoch [8/15], Step [200/500], Loss: 1.7682\n",
            "Epoch [8/15], Step [300/500], Loss: 1.9391\n",
            "Epoch [8/15], Step [400/500], Loss: 1.8859\n",
            "Epoch [8/15], Step [500/500], Loss: 1.8194\n",
            "Epoch [9/15], Step [100/500], Loss: 1.7495\n",
            "Epoch [9/15], Step [200/500], Loss: 1.6821\n",
            "Epoch [9/15], Step [300/500], Loss: 1.7374\n",
            "Epoch [9/15], Step [400/500], Loss: 1.7151\n",
            "Epoch [9/15], Step [500/500], Loss: 1.8913\n",
            "Epoch [10/15], Step [100/500], Loss: 1.9086\n",
            "Epoch [10/15], Step [200/500], Loss: 1.9149\n",
            "Epoch [10/15], Step [300/500], Loss: 1.7885\n",
            "Epoch [10/15], Step [400/500], Loss: 1.6850\n",
            "Epoch [10/15], Step [500/500], Loss: 1.6210\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Epoch [11/15], Step [100/500], Loss: 1.6763\n",
            "Epoch [11/15], Step [200/500], Loss: 1.5269\n",
            "Epoch [11/15], Step [300/500], Loss: 1.6755\n",
            "Epoch [11/15], Step [400/500], Loss: 1.6214\n",
            "Epoch [11/15], Step [500/500], Loss: 1.6575\n",
            "Epoch [12/15], Step [100/500], Loss: 1.6628\n",
            "Epoch [12/15], Step [200/500], Loss: 1.7802\n",
            "Epoch [12/15], Step [300/500], Loss: 1.7031\n",
            "Epoch [12/15], Step [400/500], Loss: 1.6681\n",
            "Epoch [12/15], Step [500/500], Loss: 1.7555\n",
            "Epoch [13/15], Step [100/500], Loss: 1.6437\n",
            "Epoch [13/15], Step [200/500], Loss: 1.5764\n",
            "Epoch [13/15], Step [300/500], Loss: 1.6376\n",
            "Epoch [13/15], Step [400/500], Loss: 1.7355\n",
            "Epoch [13/15], Step [500/500], Loss: 1.5862\n",
            "Epoch [14/15], Step [100/500], Loss: 1.5234\n",
            "Epoch [14/15], Step [200/500], Loss: 1.6640\n",
            "Epoch [14/15], Step [300/500], Loss: 1.4762\n",
            "Epoch [14/15], Step [400/500], Loss: 1.7758\n",
            "Epoch [14/15], Step [500/500], Loss: 1.6763\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Epoch [15/15], Step [100/500], Loss: 1.6182\n",
            "Epoch [15/15], Step [200/500], Loss: 1.5956\n",
            "Epoch [15/15], Step [300/500], Loss: 1.7810\n",
            "Epoch [15/15], Step [400/500], Loss: 1.5541\n",
            "Epoch [15/15], Step [500/500], Loss: 1.5347\n",
            "\n",
            "Mixed precision:\n",
            "Total execution time = 204.572 sec\n",
            "Max memory used by tensors = 162724864 bytes\n",
            "Files already downloaded and verified\n",
            "Accuracy of the network on the 10000 test images: 42 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O3:  Pure FP16 training.\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O3\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : False\n",
            "master_weights         : False\n",
            "loss_scale             : 1.0\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O3\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : False\n",
            "master_weights         : False\n",
            "loss_scale             : 1.0\n",
            "Files already downloaded and verified\n",
            "Epoch [1/15], Step [100/500], Loss: 7.0409\n",
            "Epoch [1/15], Step [200/500], Loss: 6.8294\n",
            "Epoch [1/15], Step [300/500], Loss: 6.5850\n",
            "Epoch [1/15], Step [400/500], Loss: 6.4001\n",
            "Epoch [1/15], Step [500/500], Loss: 6.1566\n",
            "Epoch [2/15], Step [100/500], Loss: 5.8349\n",
            "Epoch [2/15], Step [200/500], Loss: 5.5495\n",
            "Epoch [2/15], Step [300/500], Loss: 5.1978\n",
            "Epoch [2/15], Step [400/500], Loss: 5.1349\n",
            "Epoch [2/15], Step [500/500], Loss: 4.9295\n",
            "Epoch [3/15], Step [100/500], Loss: 4.7122\n",
            "Epoch [3/15], Step [200/500], Loss: 4.4653\n",
            "Epoch [3/15], Step [300/500], Loss: 4.2619\n",
            "Epoch [3/15], Step [400/500], Loss: 4.2164\n",
            "Epoch [3/15], Step [500/500], Loss: 3.9607\n",
            "Epoch [4/15], Step [100/500], Loss: 3.9320\n",
            "Epoch [4/15], Step [200/500], Loss: 3.5850\n",
            "Epoch [4/15], Step [300/500], Loss: 3.5993\n",
            "Epoch [4/15], Step [400/500], Loss: 3.4297\n",
            "Epoch [4/15], Step [500/500], Loss: 3.3723\n",
            "Epoch [5/15], Step [100/500], Loss: 3.3171\n",
            "Epoch [5/15], Step [200/500], Loss: 3.2811\n",
            "Epoch [5/15], Step [300/500], Loss: 3.0809\n",
            "Epoch [5/15], Step [400/500], Loss: 3.0359\n",
            "Epoch [5/15], Step [500/500], Loss: 3.1697\n",
            "Epoch [6/15], Step [100/500], Loss: 2.9546\n",
            "Epoch [6/15], Step [200/500], Loss: 2.8111\n",
            "Epoch [6/15], Step [300/500], Loss: 2.8618\n",
            "Epoch [6/15], Step [400/500], Loss: 2.7391\n",
            "Epoch [6/15], Step [500/500], Loss: 2.7744\n",
            "Epoch [7/15], Step [100/500], Loss: 2.7182\n",
            "Epoch [7/15], Step [200/500], Loss: 2.5295\n",
            "Epoch [7/15], Step [300/500], Loss: 2.5480\n",
            "Epoch [7/15], Step [400/500], Loss: 2.6920\n",
            "Epoch [7/15], Step [500/500], Loss: 2.4674\n",
            "Epoch [8/15], Step [100/500], Loss: 2.5459\n",
            "Epoch [8/15], Step [200/500], Loss: 2.4042\n",
            "Epoch [8/15], Step [300/500], Loss: 2.5024\n",
            "Epoch [8/15], Step [400/500], Loss: 2.4794\n",
            "Epoch [8/15], Step [500/500], Loss: 2.4424\n",
            "Epoch [9/15], Step [100/500], Loss: 2.4587\n",
            "Epoch [9/15], Step [200/500], Loss: 2.3228\n",
            "Epoch [9/15], Step [300/500], Loss: 2.3017\n",
            "Epoch [9/15], Step [400/500], Loss: 2.2553\n",
            "Epoch [9/15], Step [500/500], Loss: 2.4614\n",
            "Epoch [10/15], Step [100/500], Loss: 2.5070\n",
            "Epoch [10/15], Step [200/500], Loss: 2.4205\n",
            "Epoch [10/15], Step [300/500], Loss: 2.3226\n",
            "Epoch [10/15], Step [400/500], Loss: 2.2348\n",
            "Epoch [10/15], Step [500/500], Loss: 2.1499\n",
            "Epoch [11/15], Step [100/500], Loss: 2.2501\n",
            "Epoch [11/15], Step [200/500], Loss: 2.1861\n",
            "Epoch [11/15], Step [300/500], Loss: 2.2309\n",
            "Epoch [11/15], Step [400/500], Loss: 2.1797\n",
            "Epoch [11/15], Step [500/500], Loss: 2.1767\n",
            "Epoch [12/15], Step [100/500], Loss: 2.2184\n",
            "Epoch [12/15], Step [200/500], Loss: 2.2100\n",
            "Epoch [12/15], Step [300/500], Loss: 2.0901\n",
            "Epoch [12/15], Step [400/500], Loss: 2.1239\n",
            "Epoch [12/15], Step [500/500], Loss: 2.3083\n",
            "Epoch [13/15], Step [100/500], Loss: 2.1776\n",
            "Epoch [13/15], Step [200/500], Loss: 2.2211\n",
            "Epoch [13/15], Step [300/500], Loss: 2.2358\n",
            "Epoch [13/15], Step [400/500], Loss: 2.1505\n",
            "Epoch [13/15], Step [500/500], Loss: 2.1558\n",
            "Epoch [14/15], Step [100/500], Loss: 2.0257\n",
            "Epoch [14/15], Step [200/500], Loss: 2.2184\n",
            "Epoch [14/15], Step [300/500], Loss: 2.0749\n",
            "Epoch [14/15], Step [400/500], Loss: 2.2199\n",
            "Epoch [14/15], Step [500/500], Loss: 2.1652\n",
            "Epoch [15/15], Step [100/500], Loss: 2.0642\n",
            "Epoch [15/15], Step [200/500], Loss: 2.2163\n",
            "Epoch [15/15], Step [300/500], Loss: 2.2525\n",
            "Epoch [15/15], Step [400/500], Loss: 2.1923\n",
            "Epoch [15/15], Step [500/500], Loss: 2.0583\n",
            "\n",
            "F16 precision:\n",
            "Total execution time = 187.183 sec\n",
            "Max memory used by tensors = 98697216 bytes\n",
            "Files already downloaded and verified\n",
            "Accuracy of the network on the 10000 test images: 28 %\n"
          ]
        }
      ],
      "source": [
        "from apex import amp as amp_lib\n",
        "import os\n",
        "from datetime import datetime\n",
        "import argparse\n",
        "import torch.multiprocessing as mp\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.distributed as dist\n",
        "import apex\n",
        "import gc\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('-n', '--nodes', default=1, type=int, metavar='N',\n",
        "                        help='number of data loading workers (default: 4)')\n",
        "    parser.add_argument('-g', '--gpus', default=1, type=int,\n",
        "                        help='number of gpus per node')\n",
        "    parser.add_argument('-nr', '--nr', default=0, type=int,\n",
        "                        help='ranking within the nodes')\n",
        "    parser.add_argument('--epochs', default=15, type=int, metavar='N',\n",
        "                        help='number of total epochs to run')\n",
        "    args = parser.parse_args([])\n",
        "    args.world_size = args.gpus * args.nodes\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '8888'\n",
        "\n",
        "    test(train(args, True, False), False) # Mixed precision\n",
        "    test(train(args, False, True), True) # F16 everywhere\n",
        "\n",
        "\n",
        "def train(args, amp, f16):\n",
        "    torch.manual_seed(42)\n",
        "    model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)\n",
        "    model = model.to(torch.float32)\n",
        "    model = model.cuda()\n",
        "    model.train()\n",
        "    batch_size = 100\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss().cuda()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), 1e-4)\n",
        "    if amp:\n",
        "      opt_level = \"O2\"\n",
        "    elif f16:\n",
        "      opt_level = \"O3\"\n",
        "    model, optimizer = amp_lib.initialize(model, optimizer, opt_level=opt_level)\n",
        "    train_dataset = torchvision.datasets.CIFAR10(root='./data',\n",
        "                                               train=True,\n",
        "                                               transform=transforms.ToTensor(),\n",
        "                                               download=True)\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=True,\n",
        "                                               num_workers=0,\n",
        "                                               pin_memory=True)\n",
        "\n",
        "    total_step = len(train_loader)\n",
        "    start_timer()\n",
        "    for epoch in range(args.epochs):\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images = images.cuda(non_blocking=True)\n",
        "            labels = labels.cuda(non_blocking=True)\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            with amp_lib.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                scaled_loss.backward()\n",
        "            optimizer.step()\n",
        "            if (i + 1) % batch_size == 0:\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(\n",
        "                    epoch + 1,\n",
        "                    args.epochs,\n",
        "                    i + 1,\n",
        "                    total_step,\n",
        "                    loss.item()))\n",
        "\n",
        "    if f16:\n",
        "      end_timer_and_print(\"F16 precision:\")\n",
        "    elif amp:\n",
        "      end_timer_and_print(\"Mixed precision:\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def test(model, f16, batch_size = 100):\n",
        "  model.eval()\n",
        "  test_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data',\n",
        "        train=False,\n",
        "        transform=transforms.ToTensor(),\n",
        "        download=True\n",
        "    )\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "      dataset=test_dataset,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False,\n",
        "      num_workers=0,\n",
        "      pin_memory=True,)\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "      images = images.cuda(non_blocking=True)\n",
        "      labels = labels.cuda(non_blocking=True)\n",
        "      outputs = model(images)\n",
        "\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "  print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEGgH1wOLEVD",
        "outputId": "fce6813d-c0d8-4b24-d903-69047007c3b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Epoch [1/15], Step [100/500], Loss: 6.9299\n",
            "Epoch [1/15], Step [200/500], Loss: 6.6007\n",
            "Epoch [1/15], Step [300/500], Loss: 6.2419\n",
            "Epoch [1/15], Step [400/500], Loss: 5.8822\n",
            "Epoch [1/15], Step [500/500], Loss: 5.5221\n",
            "Epoch [2/15], Step [100/500], Loss: 5.1349\n",
            "Epoch [2/15], Step [200/500], Loss: 4.6227\n",
            "Epoch [2/15], Step [300/500], Loss: 4.2473\n",
            "Epoch [2/15], Step [400/500], Loss: 4.2443\n",
            "Epoch [2/15], Step [500/500], Loss: 3.8726\n",
            "Epoch [3/15], Step [100/500], Loss: 3.6085\n",
            "Epoch [3/15], Step [200/500], Loss: 3.3341\n",
            "Epoch [3/15], Step [300/500], Loss: 3.1640\n",
            "Epoch [3/15], Step [400/500], Loss: 2.9709\n",
            "Epoch [3/15], Step [500/500], Loss: 2.7818\n",
            "Epoch [4/15], Step [100/500], Loss: 2.8493\n",
            "Epoch [4/15], Step [200/500], Loss: 2.4426\n",
            "Epoch [4/15], Step [300/500], Loss: 2.5043\n",
            "Epoch [4/15], Step [400/500], Loss: 2.4742\n",
            "Epoch [4/15], Step [500/500], Loss: 2.2990\n",
            "Epoch [5/15], Step [100/500], Loss: 2.2421\n",
            "Epoch [5/15], Step [200/500], Loss: 2.3866\n",
            "Epoch [5/15], Step [300/500], Loss: 2.1665\n",
            "Epoch [5/15], Step [400/500], Loss: 2.1189\n",
            "Epoch [5/15], Step [500/500], Loss: 2.1859\n",
            "Epoch [6/15], Step [100/500], Loss: 2.1283\n",
            "Epoch [6/15], Step [200/500], Loss: 2.0232\n",
            "Epoch [6/15], Step [300/500], Loss: 2.1493\n",
            "Epoch [6/15], Step [400/500], Loss: 1.9492\n",
            "Epoch [6/15], Step [500/500], Loss: 2.0904\n",
            "Epoch [7/15], Step [100/500], Loss: 1.9898\n",
            "Epoch [7/15], Step [200/500], Loss: 1.8283\n",
            "Epoch [7/15], Step [300/500], Loss: 1.7756\n",
            "Epoch [7/15], Step [400/500], Loss: 2.1196\n",
            "Epoch [7/15], Step [500/500], Loss: 1.8992\n",
            "Epoch [8/15], Step [100/500], Loss: 1.9720\n",
            "Epoch [8/15], Step [200/500], Loss: 1.7823\n",
            "Epoch [8/15], Step [300/500], Loss: 1.9582\n",
            "Epoch [8/15], Step [400/500], Loss: 1.9127\n",
            "Epoch [8/15], Step [500/500], Loss: 1.8551\n",
            "Epoch [9/15], Step [100/500], Loss: 1.7557\n",
            "Epoch [9/15], Step [200/500], Loss: 1.6726\n",
            "Epoch [9/15], Step [300/500], Loss: 1.7835\n",
            "Epoch [9/15], Step [400/500], Loss: 1.7292\n",
            "Epoch [9/15], Step [500/500], Loss: 1.9086\n",
            "Epoch [10/15], Step [100/500], Loss: 1.9621\n",
            "Epoch [10/15], Step [200/500], Loss: 1.9003\n",
            "Epoch [10/15], Step [300/500], Loss: 1.7951\n",
            "Epoch [10/15], Step [400/500], Loss: 1.6754\n",
            "Epoch [10/15], Step [500/500], Loss: 1.5936\n",
            "Epoch [11/15], Step [100/500], Loss: 1.7244\n",
            "Epoch [11/15], Step [200/500], Loss: 1.5133\n",
            "Epoch [11/15], Step [300/500], Loss: 1.6857\n",
            "Epoch [11/15], Step [400/500], Loss: 1.6226\n",
            "Epoch [11/15], Step [500/500], Loss: 1.6769\n",
            "Epoch [12/15], Step [100/500], Loss: 1.6670\n",
            "Epoch [12/15], Step [200/500], Loss: 1.7591\n",
            "Epoch [12/15], Step [300/500], Loss: 1.6981\n",
            "Epoch [12/15], Step [400/500], Loss: 1.5894\n",
            "Epoch [12/15], Step [500/500], Loss: 1.7304\n",
            "Epoch [13/15], Step [100/500], Loss: 1.6141\n",
            "Epoch [13/15], Step [200/500], Loss: 1.5830\n",
            "Epoch [13/15], Step [300/500], Loss: 1.6629\n",
            "Epoch [13/15], Step [400/500], Loss: 1.7433\n",
            "Epoch [13/15], Step [500/500], Loss: 1.6188\n",
            "Epoch [14/15], Step [100/500], Loss: 1.5620\n",
            "Epoch [14/15], Step [200/500], Loss: 1.6682\n",
            "Epoch [14/15], Step [300/500], Loss: 1.4854\n",
            "Epoch [14/15], Step [400/500], Loss: 1.7788\n",
            "Epoch [14/15], Step [500/500], Loss: 1.6864\n",
            "Epoch [15/15], Step [100/500], Loss: 1.5928\n",
            "Epoch [15/15], Step [200/500], Loss: 1.6614\n",
            "Epoch [15/15], Step [300/500], Loss: 1.6980\n",
            "Epoch [15/15], Step [400/500], Loss: 1.6055\n",
            "Epoch [15/15], Step [500/500], Loss: 1.5453\n",
            "\n",
            "Standart precision:\n",
            "Total execution time = 186.845 sec\n",
            "Max memory used by tensors = 167813632 bytes\n",
            "Files already downloaded and verified\n",
            "Accuracy of the network on the 10000 test images: 41 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Epoch [1/15], Step [100/500], Loss: 6.9275\n",
            "Epoch [1/15], Step [200/500], Loss: 6.5791\n",
            "Epoch [1/15], Step [300/500], Loss: 6.2466\n",
            "Epoch [1/15], Step [400/500], Loss: 5.8921\n",
            "Epoch [1/15], Step [500/500], Loss: 5.5237\n",
            "Epoch [2/15], Step [100/500], Loss: 5.1916\n",
            "Epoch [2/15], Step [200/500], Loss: 4.6644\n",
            "Epoch [2/15], Step [300/500], Loss: 4.2550\n",
            "Epoch [2/15], Step [400/500], Loss: 4.2432\n",
            "Epoch [2/15], Step [500/500], Loss: 3.9107\n",
            "Epoch [3/15], Step [100/500], Loss: 3.5985\n",
            "Epoch [3/15], Step [200/500], Loss: 3.3166\n",
            "Epoch [3/15], Step [300/500], Loss: 3.1856\n",
            "Epoch [3/15], Step [400/500], Loss: 2.9745\n",
            "Epoch [3/15], Step [500/500], Loss: 2.8188\n",
            "Epoch [4/15], Step [100/500], Loss: 2.8825\n",
            "Epoch [4/15], Step [200/500], Loss: 2.4484\n",
            "Epoch [4/15], Step [300/500], Loss: 2.5785\n",
            "Epoch [4/15], Step [400/500], Loss: 2.5000\n",
            "Epoch [4/15], Step [500/500], Loss: 2.3009\n",
            "Epoch [5/15], Step [100/500], Loss: 2.1745\n",
            "Epoch [5/15], Step [200/500], Loss: 2.3812\n",
            "Epoch [5/15], Step [300/500], Loss: 2.1984\n",
            "Epoch [5/15], Step [400/500], Loss: 2.0865\n",
            "Epoch [5/15], Step [500/500], Loss: 2.2274\n",
            "Epoch [6/15], Step [100/500], Loss: 2.1521\n",
            "Epoch [6/15], Step [200/500], Loss: 1.9919\n",
            "Epoch [6/15], Step [300/500], Loss: 2.1183\n",
            "Epoch [6/15], Step [400/500], Loss: 1.9826\n",
            "Epoch [6/15], Step [500/500], Loss: 2.0999\n",
            "Epoch [7/15], Step [100/500], Loss: 2.0311\n",
            "Epoch [7/15], Step [200/500], Loss: 1.8302\n",
            "Epoch [7/15], Step [300/500], Loss: 1.7716\n",
            "Epoch [7/15], Step [400/500], Loss: 2.0742\n",
            "Epoch [7/15], Step [500/500], Loss: 1.8726\n",
            "Epoch [8/15], Step [100/500], Loss: 1.9678\n",
            "Epoch [8/15], Step [200/500], Loss: 1.8207\n",
            "Epoch [8/15], Step [300/500], Loss: 1.9081\n",
            "Epoch [8/15], Step [400/500], Loss: 1.8786\n",
            "Epoch [8/15], Step [500/500], Loss: 1.8231\n",
            "Epoch [9/15], Step [100/500], Loss: 1.7571\n",
            "Epoch [9/15], Step [200/500], Loss: 1.7550\n",
            "Epoch [9/15], Step [300/500], Loss: 1.7874\n",
            "Epoch [9/15], Step [400/500], Loss: 1.7534\n",
            "Epoch [9/15], Step [500/500], Loss: 1.9196\n",
            "Epoch [10/15], Step [100/500], Loss: 1.9315\n",
            "Epoch [10/15], Step [200/500], Loss: 1.9023\n",
            "Epoch [10/15], Step [300/500], Loss: 1.7606\n",
            "Epoch [10/15], Step [400/500], Loss: 1.6653\n",
            "Epoch [10/15], Step [500/500], Loss: 1.6560\n",
            "Epoch [11/15], Step [100/500], Loss: 1.7886\n",
            "Epoch [11/15], Step [200/500], Loss: 1.5502\n",
            "Epoch [11/15], Step [300/500], Loss: 1.6922\n",
            "Epoch [11/15], Step [400/500], Loss: 1.6352\n",
            "Epoch [11/15], Step [500/500], Loss: 1.6668\n",
            "Epoch [12/15], Step [100/500], Loss: 1.6763\n",
            "Epoch [12/15], Step [200/500], Loss: 1.7126\n",
            "Epoch [12/15], Step [300/500], Loss: 1.7052\n",
            "Epoch [12/15], Step [400/500], Loss: 1.6468\n",
            "Epoch [12/15], Step [500/500], Loss: 1.7269\n",
            "Epoch [13/15], Step [100/500], Loss: 1.6202\n",
            "Epoch [13/15], Step [200/500], Loss: 1.5821\n",
            "Epoch [13/15], Step [300/500], Loss: 1.6337\n",
            "Epoch [13/15], Step [400/500], Loss: 1.7349\n",
            "Epoch [13/15], Step [500/500], Loss: 1.6291\n",
            "Epoch [14/15], Step [100/500], Loss: 1.5559\n",
            "Epoch [14/15], Step [200/500], Loss: 1.6580\n",
            "Epoch [14/15], Step [300/500], Loss: 1.4374\n",
            "Epoch [14/15], Step [400/500], Loss: 1.7767\n",
            "Epoch [14/15], Step [500/500], Loss: 1.6787\n",
            "Epoch [15/15], Step [100/500], Loss: 1.6812\n",
            "Epoch [15/15], Step [200/500], Loss: 1.5935\n",
            "Epoch [15/15], Step [300/500], Loss: 1.8049\n",
            "Epoch [15/15], Step [400/500], Loss: 1.6541\n",
            "Epoch [15/15], Step [500/500], Loss: 1.5309\n",
            "\n",
            "Mixed precision:\n",
            "Total execution time = 208.011 sec\n",
            "Max memory used by tensors = 139707904 bytes\n",
            "Files already downloaded and verified\n",
            "Accuracy of the network on the 10000 test images: 41 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Epoch [1/15], Step [100/500], Loss: 7.0600\n",
            "Epoch [1/15], Step [200/500], Loss: 6.8269\n",
            "Epoch [1/15], Step [300/500], Loss: 6.5725\n",
            "Epoch [1/15], Step [400/500], Loss: 6.4223\n",
            "Epoch [1/15], Step [500/500], Loss: 6.1407\n",
            "Epoch [2/15], Step [100/500], Loss: 5.9111\n",
            "Epoch [2/15], Step [200/500], Loss: 5.5568\n",
            "Epoch [2/15], Step [300/500], Loss: 5.2705\n",
            "Epoch [2/15], Step [400/500], Loss: 5.1285\n",
            "Epoch [2/15], Step [500/500], Loss: 5.0193\n",
            "Epoch [3/15], Step [100/500], Loss: 4.6782\n",
            "Epoch [3/15], Step [200/500], Loss: 4.4520\n",
            "Epoch [3/15], Step [300/500], Loss: 4.3200\n",
            "Epoch [3/15], Step [400/500], Loss: 4.1896\n",
            "Epoch [3/15], Step [500/500], Loss: 4.0132\n",
            "Epoch [4/15], Step [100/500], Loss: 3.9109\n",
            "Epoch [4/15], Step [200/500], Loss: 3.5569\n",
            "Epoch [4/15], Step [300/500], Loss: 3.5630\n",
            "Epoch [4/15], Step [400/500], Loss: 3.4555\n",
            "Epoch [4/15], Step [500/500], Loss: 3.3631\n",
            "Epoch [5/15], Step [100/500], Loss: 3.3713\n",
            "Epoch [5/15], Step [200/500], Loss: 3.2780\n",
            "Epoch [5/15], Step [300/500], Loss: 3.0819\n",
            "Epoch [5/15], Step [400/500], Loss: 2.9960\n",
            "Epoch [5/15], Step [500/500], Loss: 3.1401\n",
            "Epoch [6/15], Step [100/500], Loss: 2.9620\n",
            "Epoch [6/15], Step [200/500], Loss: 2.7369\n",
            "Epoch [6/15], Step [300/500], Loss: 2.8281\n",
            "Epoch [6/15], Step [400/500], Loss: 2.6925\n",
            "Epoch [6/15], Step [500/500], Loss: 2.7533\n",
            "Epoch [7/15], Step [100/500], Loss: 2.7350\n",
            "Epoch [7/15], Step [200/500], Loss: 2.5158\n",
            "Epoch [7/15], Step [300/500], Loss: 2.5711\n",
            "Epoch [7/15], Step [400/500], Loss: 2.6250\n",
            "Epoch [7/15], Step [500/500], Loss: 2.5234\n",
            "Epoch [8/15], Step [100/500], Loss: 2.6014\n",
            "Epoch [8/15], Step [200/500], Loss: 2.3916\n",
            "Epoch [8/15], Step [300/500], Loss: 2.4526\n",
            "Epoch [8/15], Step [400/500], Loss: 2.5203\n",
            "Epoch [8/15], Step [500/500], Loss: 2.3531\n",
            "Epoch [9/15], Step [100/500], Loss: 2.5007\n",
            "Epoch [9/15], Step [200/500], Loss: 2.3218\n",
            "Epoch [9/15], Step [300/500], Loss: 2.3450\n",
            "Epoch [9/15], Step [400/500], Loss: 2.3275\n",
            "Epoch [9/15], Step [500/500], Loss: 2.5207\n",
            "Epoch [10/15], Step [100/500], Loss: 2.4368\n",
            "Epoch [10/15], Step [200/500], Loss: 2.4205\n",
            "Epoch [10/15], Step [300/500], Loss: 2.3294\n",
            "Epoch [10/15], Step [400/500], Loss: 2.2846\n",
            "Epoch [10/15], Step [500/500], Loss: 2.1843\n",
            "Epoch [11/15], Step [100/500], Loss: 2.2561\n",
            "Epoch [11/15], Step [200/500], Loss: 2.1913\n",
            "Epoch [11/15], Step [300/500], Loss: 2.1144\n",
            "Epoch [11/15], Step [400/500], Loss: 2.1555\n",
            "Epoch [11/15], Step [500/500], Loss: 2.1825\n",
            "Epoch [12/15], Step [100/500], Loss: 2.2466\n",
            "Epoch [12/15], Step [200/500], Loss: 2.2011\n",
            "Epoch [12/15], Step [300/500], Loss: 2.0483\n",
            "Epoch [12/15], Step [400/500], Loss: 2.0852\n",
            "Epoch [12/15], Step [500/500], Loss: 2.4083\n",
            "Epoch [13/15], Step [100/500], Loss: 2.1575\n",
            "Epoch [13/15], Step [200/500], Loss: 2.1516\n",
            "Epoch [13/15], Step [300/500], Loss: 2.2646\n",
            "Epoch [13/15], Step [400/500], Loss: 2.1667\n",
            "Epoch [13/15], Step [500/500], Loss: 2.1669\n",
            "Epoch [14/15], Step [100/500], Loss: 2.0517\n",
            "Epoch [14/15], Step [200/500], Loss: 2.1317\n",
            "Epoch [14/15], Step [300/500], Loss: 2.0724\n",
            "Epoch [14/15], Step [400/500], Loss: 2.2131\n",
            "Epoch [14/15], Step [500/500], Loss: 2.1112\n",
            "Epoch [15/15], Step [100/500], Loss: 2.0212\n",
            "Epoch [15/15], Step [200/500], Loss: 2.1745\n",
            "Epoch [15/15], Step [300/500], Loss: 2.1972\n",
            "Epoch [15/15], Step [400/500], Loss: 2.1794\n",
            "Epoch [15/15], Step [500/500], Loss: 2.0861\n",
            "\n",
            "F16 precision:\n",
            "Total execution time = 189.080 sec\n",
            "Max memory used by tensors = 92843008 bytes\n",
            "Files already downloaded and verified\n",
            "Accuracy of the network on the 10000 test images: 28 %\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import argparse\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from torch import nn\n",
        "import gc\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('-n', '--nodes', default=1, type=int, metavar='N',\n",
        "                        help='number of data loading workers (default: 4)')\n",
        "    parser.add_argument('-g', '--gpus', default=1, type=int,\n",
        "                        help='number of gpus per node')\n",
        "    parser.add_argument('-nr', '--nr', default=0, type=int,\n",
        "                        help='ranking within the nodes')\n",
        "    parser.add_argument('--epochs', default=15, type=int, metavar='N',\n",
        "                        help='number of total epochs to run')\n",
        "    args = parser.parse_args([])\n",
        "    args.world_size = args.gpus * args.nodes\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '8888'\n",
        "\n",
        "    test(train(args, False, False), False) # Standard\n",
        "    test(train(args, True, False), False) # Mixed precision\n",
        "    test(train(args, False, True), True) # F16 everywhere\n",
        "\n",
        "\n",
        "def train(args, amp, f16):\n",
        "    torch.manual_seed(42)\n",
        "    model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)\n",
        "\n",
        "    if f16:\n",
        "      model = model.to(torch.float16)\n",
        "    else:\n",
        "      model = model.to(torch.float32)\n",
        "\n",
        "    model = model.cuda()\n",
        "    model.train()\n",
        "    batch_size = 100\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss().cuda()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), 1e-4)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
        "    train_dataset = torchvision.datasets.CIFAR10(root='./data',\n",
        "                                               train=True,\n",
        "                                               transform=transforms.ToTensor(),\n",
        "                                               download=True)\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=True,\n",
        "                                               num_workers=0,\n",
        "                                               pin_memory=True)\n",
        "\n",
        "    total_step = len(train_loader)\n",
        "    start_timer()\n",
        "    for epoch in range(args.epochs):\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            with torch.autocast(\"cuda\", dtype=torch.float16, enabled=(amp or f16)):\n",
        "              images = images.cuda(non_blocking=True)\n",
        "              labels = labels.cuda(non_blocking=True)\n",
        "              output = model(images)\n",
        "              loss = criterion(output, labels)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            if (i + 1) % batch_size == 0:\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, args.epochs, i + 1, total_step, loss.item()))\n",
        "\n",
        "    if f16:\n",
        "      end_timer_and_print(\"F16 precision:\")\n",
        "    elif amp:\n",
        "      end_timer_and_print(\"Mixed precision:\")\n",
        "    else:\n",
        "      end_timer_and_print(\"Standart precision:\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def test(model, f16, batch_size = 100):\n",
        "  model.eval()\n",
        "  if f16:\n",
        "    model = model.to(torch.float16)\n",
        "  else:\n",
        "    model = model.to(torch.float32)\n",
        "\n",
        "  test_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data',\n",
        "        train=False,\n",
        "        transform=transforms.ToTensor(),\n",
        "        download=True\n",
        "    )\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "      dataset=test_dataset,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False,\n",
        "      num_workers=0,\n",
        "      pin_memory=True,)\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "      for images, labels in test_loader:\n",
        "        with torch.autocast(\"cuda\", dtype=torch.float16, enabled=f16):\n",
        "          images = images.cuda(non_blocking=True)\n",
        "          labels = labels.cuda(non_blocking=True)\n",
        "          outputs = model(images)\n",
        "\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "  print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
